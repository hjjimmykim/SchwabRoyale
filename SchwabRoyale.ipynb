{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SchwabRoyale.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "oIo5MwsnKqOU",
        "9gxqu8GD7wlo",
        "i9BFTN_2_pVC",
        "WOm2e3M393BK"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hjjimmykim/SchwabRoyale/blob/master/SchwabRoyale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "n_BXMiSjENN-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Schwab Royale\n",
        "\n",
        "In the beginning there was darkness.\n",
        "Then there was David."
      ]
    },
    {
      "metadata": {
        "id": "mr1q4B6P18vs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ]
    },
    {
      "metadata": {
        "id": "N5jfqjoT2OrU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "dcc4962a-1f35-4b68-bcf4-6adc99bb5c8e"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "import time\n",
        "import copy\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "#from graphics import *"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IH8FIvLJHeDW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ]
    },
    {
      "metadata": {
        "id": "53uGs_23HfR_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Agent parameters\n",
        "K = 4           # Number of teams\n",
        "N = 4           # Number of agents per team\n",
        "hp_start = 2    # Starting hp\n",
        "action_size = 4 # Four cardinal directions\n",
        "\n",
        "# Simulation parameters\n",
        "M = 32    # Width of map\n",
        "max_turn = 10000 # Maximum number of turns\n",
        "n_ep = 2  # Number of training episodes (games)\n",
        "\n",
        "# Reinforcement learning parameters\n",
        "batch_size = 100  # Batch size\n",
        "memory_size = 10000 # Number of experiences agent can keep\n",
        "hidden_dim = 100 # Hidden layer size\n",
        "\n",
        "target_copy_freq = 10 # Update the target network every this many turns\n",
        "\n",
        "alpha = 0.01 # Learning rate\n",
        "beta = 0.1   # Exploitation parameter\n",
        "gamma = 0.9  # Discount factor\n",
        "sigma = 0.5  # Selfishness factor\n",
        "Omega = N    # Final survivor bonus (should it be for team and/or self?)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HDXNiVV_ssWR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Spunk\n",
        "\n",
        "Constantijn takes responsibility for the time wasted making this, however, Jimmy supported him."
      ]
    },
    {
      "metadata": {
        "id": "CeExgoRAspoP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "death_messages = [\"was wasted by\",\n",
        "                  \"met a bitter end at the hands of\",\n",
        "                  \"was swarmed by TensorBros sent by\",\n",
        "                  \"was forced to press Alt+F4 by\",\n",
        "                  \"was peer-reviewed by\",\n",
        "                  \"got schwabbed by\", # https://www.urbandictionary.com/define.php?term=Schwabbed\n",
        "                  \"was brutally neglected by\",\n",
        "                  \"had his heart stolen by\",\n",
        "                  \"slipped and fell while running from\",\n",
        "                  \"became petrified by the sight of\"\n",
        "                 ]\n",
        "\n",
        "def kill_log(killer, killer_team, victim, victim_team, turn):\n",
        "  phrase = \"%s [Team %i] \" + np.random.choice(death_messages) + \" %s [Team %i] on turn %i.\"\n",
        "  return phrase % (\"David \"+str(victim), victim_team, \"David \"+str(killer), killer_team, turn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U_l6IAIO-hd4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "VesLnp7DMAOB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "*   https://web.stanford.edu/class/cs20si/2017/lectures/slides_14.pdf\n",
        "*   https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Deep%20Q%20Learning/Doom/Deep%20Q%20learning%20with%20Doom.ipynb\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "PV0aOsNe-kQh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Net:\n",
        "  def __init__(self, input_dim, hidden_dim = 100, output_dim = 4):\n",
        "    # Fully-connected feedforward network with 2 hidden layers    \n",
        "    # input-dim = 1-D reshaped map + hp\n",
        "    # hidden_dim = number of units per hidden layer\n",
        "    # output_dim = number of outputs (i.e. actions)\n",
        "\n",
        "    # Input placeholder\n",
        "    self.inputs = tf.placeholder(tf.float32, shape=[None,input_dim], name=\"inputs\")\n",
        "\n",
        "    ## Weight initialization\n",
        "    # input -> h1\n",
        "    W0 = tf.Variable(tf.truncated_normal([input_dim,hidden_dim],stddev=0.1))\n",
        "    b0 = tf.Variable(tf.constant(0.1,shape=[hidden_dim]))\n",
        "\n",
        "    # h1 -> h2\n",
        "    W1 = tf.Variable(tf.truncated_normal([hidden_dim,hidden_dim],stddev=0.1))\n",
        "    b1 = tf.Variable(tf.constant(0.1,shape=[hidden_dim]))\n",
        "    \n",
        "    # h2 -> output\n",
        "    W2 = tf.Variable(tf.truncated_normal([hidden_dim,output_dim],stddev=0.1))\n",
        "    b2 = tf.Variable(tf.constant(0.1,shape=[output_dim]))\n",
        "    \n",
        "    \n",
        "    ## Hidden layers\n",
        "    h1 = tf.nn.relu(tf.matmul(self.inputs,W0) + b0)\n",
        "    h2 = tf.nn.relu(tf.matmul(h1,W1) + b1)\n",
        "    \n",
        "    ## Output layer\n",
        "    self.outputs = tf.matmul(h2,W2) + b2\n",
        "    \n",
        "    ## Keep track of parameters\n",
        "    self.params = [W0,b0,W1,b1,W2,b2]\n",
        "    \n",
        "    \n",
        "    #Learnability\n",
        "    self.actions = tf.placeholder(tf.float32, shape=[None, action_size], name=\"actions\")\n",
        "    \n",
        "    self.Q = tf.reduce_sum(tf.multiply(self.outputs, self.actions), axis=1)\n",
        "    self.target_Q = tf.placeholder(tf.float32, [None])\n",
        "    \n",
        "    self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))    \n",
        "    self.optimizer = tf.train.RMSPropOptimizer(alpha).minimize(self.loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SyLYUKe-JNTL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Memory Class source: https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb"
      ]
    },
    {
      "metadata": {
        "id": "dI-HynsoJMgH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Memory:\n",
        "  def __init__(self, max_size):\n",
        "    self.max = max_size\n",
        "    self.buffer = deque(maxlen = max_size)    # \"deque\" = Double Ended QUEue\n",
        "  \n",
        "  # Add entry. FIFO.\n",
        "  def add(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "  \n",
        "  # Sample batch_size number of entries, without replacement. Return as a list.\n",
        "  def sample(self, batch_size):\n",
        "    buffer_size = len(self.buffer)\n",
        "    index = np.random.choice(np.arange(buffer_size),\n",
        "                             size = batch_size,\n",
        "                             replace = False)\n",
        "    return [self.buffer[i] for i in index]\n",
        "  \n",
        "  # Own additions\n",
        "  def is_full(self):\n",
        "    return len(self.buffer) == self.max\n",
        "  \n",
        "  def wipe(self):\n",
        "    self.buffer.clear() # NB, does not affect max length\n",
        "\n",
        "  def length(self):\n",
        "    return len(self.buffer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oIo5MwsnKqOU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Stanford fancy memory alternative:"
      ]
    },
    {
      "metadata": {
        "id": "tGZ4T3gLLqqY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "  \n",
        "  def __init__(self, template, capacity):\n",
        "    self._capacity = capacity # Number of memories to maintain\n",
        "    self._buffers = self._create_buffers(template)\n",
        "    self._index = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
        "    \n",
        "  # Number of memories saved\n",
        "  def size(self):\n",
        "    return tf.minimum(self._index, self._capacity)\n",
        "  \n",
        "  def append(self, tensors):\n",
        "    position = tf.mod(self._index, self._capacity)\n",
        "    with tf.control_dependencies([\n",
        "        b[position].assign(t) for b, t in\n",
        "        zip(self._buffers, tensors)]):\n",
        "      return self._index.assign_add(1)\n",
        "  \n",
        "  def sample(self, amount):\n",
        "    positions = tf.random_uniform((amount,), 0, self.size - 1, tf.int32)\n",
        "    return [tf.gather(b, positions)\n",
        "            for b in self._buffers]\n",
        "\n",
        "  def _create_buffers(self, template):\n",
        "    buffers = []\n",
        "    for tensor in template:\n",
        "      shape = tf.TensorShape([self._capacity]).concatenate(\n",
        "          tensor.get_shape())\n",
        "      initial = tf.zeros(shape, tensor.dtype)\n",
        "      buffers.append(tf.Variable(\n",
        "          initial, trainable=False))\n",
        "    return buffers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lmn1TntO3jey",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Agent"
      ]
    },
    {
      "metadata": {
        "id": "8B0tnXDx3d7o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self, id, hp, loc, k, memory_size, tenure):\n",
        "    self.id = id    # Agent id (in agent_dict)\n",
        "    self.hp = hp    # Hit points\n",
        "    self.loc = loc  # Location (x,y coordinate)\n",
        "    self.team = k   # Team index (k = 1, ..., K), since 0 is self!\n",
        "    self.memory_size = memory_size # Experience replay maximum capacity\n",
        "    self.tenure = tenure       # If you don't have tenure yet, you gotta learn (i.e. determines whether the agent is learning)\n",
        "    \n",
        "    self.personal_reward = 0   # Keep track of personal reward    \n",
        "    \n",
        "    # Create brain\n",
        "    input_dim = M*M + 1 # Map cells + hp\n",
        "    output_dim = action_size\n",
        "    \n",
        "    self.DQN = Net(input_dim, hidden_dim, output_dim) # Personal neural network\n",
        "    #self.DQN_target = copy.deepcopy(self.DQN)         # Target network\n",
        "    \n",
        "    self.memory = Memory(memory_size)\n",
        "    \n",
        "  \n",
        "  # Checks living status\n",
        "  def alive(self):\n",
        "    return self.hp > 0\n",
        "  \n",
        "  \n",
        "  # State formation\n",
        "  def observe(self, map):\n",
        "    state = copy.deepcopy(map)\n",
        "    state[self.loc[0]][self.loc[1]] = 0            # Own location = 0 on map\n",
        "    state = np.reshape(state, [1,-1]).squeeze()    # Convert to 1D array\n",
        "    state = np.hstack([state, self.hp])            # Append hp\n",
        "    return state\n",
        "  '''\n",
        "  # Takes a state, and returns an action from the action space\n",
        "  def choose_action(self, state, sess):\n",
        "    # Action selection (Q function) ------------------------------------------\n",
        "    q_values = self.DQN(state) # Get q-values from the network\n",
        "    p_values = tf.exp(beta * q_values)/tf.reduce_sum(tf.exp(beta * q_values)) # Boltzmann probabilities\n",
        "    \n",
        "    action = np.random.choice(np.arange(4),p=p_values.eval())\n",
        "    return action\n",
        "  '''\n",
        "  \n",
        "  # Taking an action based on chosen direction\n",
        "  def act(self, dir, turn, quiet):    # dir should be an np.array\n",
        "    # Rewards resulting from this move\n",
        "    personal_point = 0\n",
        "    team_point = 0\n",
        "    \n",
        "    target_loc = self.loc + dir # Candidate target location\n",
        "    \n",
        "    # Check if target location is within bounds and the agent actually moves\n",
        "    if check_valid_loc(target_loc) and ~np.all(dir == [0,0]):\n",
        "      \n",
        "      target_ind = map[target_loc[0]][target_loc[1]]    # Object at target location\n",
        "      \n",
        "      if target_ind == -1:                              # If target location is empty\n",
        "        map[self.loc[0],self.loc[1]] = -1               # Previous location becomes empty\n",
        "        map[target_loc[0],target_loc[1]] = self.team    # Target location becomes occupied\n",
        "        self.loc = target_loc                           # Update location\n",
        "      else:                                             # If target location is occupied\n",
        "        # Agent at target location\n",
        "        target_id = find_agent(target_loc)\n",
        "        target_agent = agent_dict[target_id]\n",
        "        '''\n",
        "        if not target_agent.alive(): # Target agent is already dead; this should not run\n",
        "          print(\"David\", self.id, \"teabagged David\", target_id)\n",
        "        '''\n",
        "        target_agent.hp -= 1                            # Deal damage\n",
        "\n",
        "        if not target_agent.alive():                    # If target agent has been killed\n",
        "          # Display kill log\n",
        "          if not quiet:\n",
        "            print(kill_log(self.id, self.team, target_id, target_agent.team, turn))\n",
        "\n",
        "          target_agent.loc = [-1,-1]                    # Move corpse to the underworld\n",
        "\n",
        "          map[self.loc[0],self.loc[1]] = -1             # Previous location becomes empty\n",
        "          map[target_loc[0],target_loc[1]] = self.team  # Target location becomes occupied\n",
        "          self.loc = target_loc                         # Update location\n",
        "          \n",
        "          personal_point += 1             # Gain a point for self\n",
        "          if target_agent.team != self.team:\n",
        "            team_point += 1               # Gain a point for the team, if the target was from another team\n",
        "  \n",
        "    # Update the cumulative rewards\n",
        "    self.personal_reward += personal_point\n",
        "    team_scores[self.team-1] += team_point\n",
        "    \n",
        "    # Return immediate rewards\n",
        "    return personal_point, team_point"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9gxqu8GD7wlo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Agent Testing"
      ]
    },
    {
      "metadata": {
        "id": "rAXGZJB3mryV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "N.B.: Before testing agent features, run Initialize below"
      ]
    },
    {
      "metadata": {
        "id": "i9BFTN_2_pVC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Movement Testing\n",
        "\n",
        "Test whether movement is recorded on the map properly, \n",
        "test agent's functions"
      ]
    },
    {
      "metadata": {
        "id": "HezoyymCjO8I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "initialize_map()\n",
        "test_agent = agent_dict[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U-9rGq367AZH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "12193ca4-2e7b-4266-eba0-33d025287f2a"
      },
      "cell_type": "code",
      "source": [
        "agent_dict"
      ],
      "execution_count": 346,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: <__main__.Agent at 0x7f42fdccefd0>,\n",
              " 1: <__main__.Agent at 0x7f42fdcce668>,\n",
              " 2: <__main__.Agent at 0x7f42fdc90940>,\n",
              " 3: <__main__.Agent at 0x7f42fdc905c0>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 346
        }
      ]
    },
    {
      "metadata": {
        "id": "1Iu1y-TjjoTU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "2a83be2a-8292-4be7-ee2c-2e95257bea1d"
      },
      "cell_type": "code",
      "source": [
        "print(test_agent.loc)\n",
        "test_agent.observe()"
      ],
      "execution_count": 347,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0, -1,  1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [ 2, -1,  2, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 347
        }
      ]
    },
    {
      "metadata": {
        "id": "ISh3mw8lkJdh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "7ebe79af-6e57-457d-982e-f99e732a8386"
      },
      "cell_type": "code",
      "source": [
        "test_agent.move([1,1])\n",
        "print(test_agent.loc)\n",
        "test_agent.observe()"
      ],
      "execution_count": 348,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1, -1,  1, -1, -1],\n",
              "       [-1,  0, -1, -1, -1],\n",
              "       [ 2, -1,  2, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 348
        }
      ]
    },
    {
      "metadata": {
        "id": "ojTVFGTo6Dod",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "f28872c7-fbf2-4ceb-97e2-c6362cae84a3"
      },
      "cell_type": "code",
      "source": [
        "test_agent.move([0,-1])\n",
        "print(test_agent.loc)\n",
        "test_agent.observe()"
      ],
      "execution_count": 349,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1, -1,  1, -1, -1],\n",
              "       [ 0, -1, -1, -1, -1],\n",
              "       [ 2, -1,  2, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 349
        }
      ]
    },
    {
      "metadata": {
        "id": "cH_hnwXP6I3K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "0f0cdada-a2ab-446c-d6f0-7352094ad22f"
      },
      "cell_type": "code",
      "source": [
        "test_agent.move([-1,0])\n",
        "print(test_agent.loc)\n",
        "test_agent.observe()"
      ],
      "execution_count": 350,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0, -1,  1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [ 2, -1,  2, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 350
        }
      ]
    },
    {
      "metadata": {
        "id": "Z6_2HK0EthxI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8d26b993-714c-4a5e-8a72-b8fc1d669705"
      },
      "cell_type": "code",
      "source": [
        "find_agent([0,0])"
      ],
      "execution_count": 351,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 351
        }
      ]
    },
    {
      "metadata": {
        "id": "WOm2e3M393BK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Combat Testing\n",
        "\n",
        "Test attacking (movement upon kill), HP tracking, death of a teammate (personal reward), and death of an enemy (personal and team reward)"
      ]
    },
    {
      "metadata": {
        "id": "_oIwAM9E9-1C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "initialize_map()\n",
        "attack_agent = agent_dict[0]\n",
        "gimp_agent = agent_dict[1]\n",
        "enemy_agent = agent_dict[3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sYW8cZbA-Jni",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "88ed7a1b-f2dd-4902-eedd-65e4889dd733"
      },
      "cell_type": "code",
      "source": [
        "print(\"Attacker's HP:\", attack_agent.hp)\n",
        "print(\"Gimp's HP:\", gimp_agent.hp)\n",
        "print(\"Attacker's Score:\", attack_agent.reward)    # Just to check it's well-behaved\n",
        "print(\"Team Scores\", team_scores)\n",
        "attack_agent.observe()"
      ],
      "execution_count": 434,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attacker's HP: 2\n",
            "Gimp's HP: 2\n",
            "Attacker's Score: 0\n",
            "Team Scores [0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0, -1,  1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [ 2, -1,  2, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 434
        }
      ]
    },
    {
      "metadata": {
        "id": "QU7XuuVt-YOr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "02e57987-53ce-4cc1-bcd4-2b799f73d642"
      },
      "cell_type": "code",
      "source": [
        "attack_agent.move([0,2])\n",
        "print(\"Gimp's HP:\", gimp_agent.hp)\n",
        "print(\"Attacker's Score:\", attack_agent.reward)\n",
        "print(\"Team Scores\", team_scores)\n",
        "attack_agent.observe()"
      ],
      "execution_count": 435,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gimp's HP: 1\n",
            "Attacker's Score: 0\n",
            "Team Scores [0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0, -1,  1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [ 2, -1,  2, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 435
        }
      ]
    },
    {
      "metadata": {
        "id": "v28ctonPApve",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "8dcde36f-35db-4d6f-f64b-f195ecdbb453"
      },
      "cell_type": "code",
      "source": [
        "attack_agent.move([0,2])\n",
        "print(\"Gimp's HP:\", gimp_agent.hp)\n",
        "print(\"Attacker's Score:\", attack_agent.reward)\n",
        "print(\"Team Scores\", team_scores)\n",
        "attack_agent.observe()"
      ],
      "execution_count": 436,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gimp's HP: 0\n",
            "Attacker's Score: 1\n",
            "Team Scores [0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1, -1,  0, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [ 2, -1,  2, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 436
        }
      ]
    },
    {
      "metadata": {
        "id": "66btVJboCzed",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "472fb169-39bf-4393-9b9e-24e128015c80"
      },
      "cell_type": "code",
      "source": [
        "attack_agent.move([2,0])\n",
        "attack_agent.move([2,0])\n",
        "print(\"Enemy's HP:\", enemy_agent.hp)\n",
        "print(\"Attacker's Score:\", attack_agent.reward)\n",
        "print(\"Team Scores\", team_scores)\n",
        "attack_agent.observe()"
      ],
      "execution_count": 437,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enemy's HP: 0\n",
            "Attacker's Score: 2\n",
            "Team Scores [1 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1, -1, -1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [ 2, -1,  0, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 437
        }
      ]
    },
    {
      "metadata": {
        "id": "hNcirfSL_MpA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b5f6db64-7ceb-492a-d0c4-103a4182fc3a"
      },
      "cell_type": "code",
      "source": [
        "agent_dict"
      ],
      "execution_count": 438,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: <__main__.Agent at 0x7f42fdc64630>, 2: <__main__.Agent at 0x7f42fdc645c0>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 438
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "M8oGCzf5I-Bz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M0J2q94t42RL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ]
    },
    {
      "metadata": {
        "id": "7LrfjkLfJKKG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Schwifty Functions\n",
        "# Check if the given location is within bounds\n",
        "def check_valid_loc(loc):\n",
        "  return (np.all(0 <= loc) and np.all(loc < M))\n",
        "\n",
        "# Find the agent at a given location\n",
        "def find_agent(loc):\n",
        "  for ind, agnt in agent_dict.items(): # Loop through all agents\n",
        "    if np.all(agnt.loc == loc): # Return the index of agent at the location\n",
        "      return ind\n",
        "# Translates action space to a direction\n",
        "def get_dir(self, action):\n",
        "  if action == 0:                 # North\n",
        "    dir = np.array([-1,0])\n",
        "  elif action == 1:               # East\n",
        "    dir = np.array([0,1])\n",
        "  elif action == 2:               # West\n",
        "    dir = np.array([0,-1])\n",
        "  elif action  == 3:              # South\n",
        "    dir = np.array([1,0])\n",
        "  return dir\n",
        "    \n",
        "\n",
        "# Initialize map, agents, and team scores\n",
        "def initialize():\n",
        "  agent_dict = {}                            # Dictionary of all agents\n",
        "  map = -np.ones([M,M],dtype=np.int)         # M x M map\n",
        "  team_scores = np.zeros(K, dtype=np.int)    # Team scores\n",
        "  #experience_replays = []                    # Experience replays (for Q-learning)\n",
        "\n",
        "  for k in range(1, K+1):    # Loop through teams\n",
        "    for n in range(N):       # Loop through agents per team\n",
        "      id = N*(k-1) + n                                   # Agent id\n",
        "      loc = np.array([int(M/K)*(k-1),int(M/N)*n])        # Starting location\n",
        "  \n",
        "      agent_dict[id] = Agent(id, hp_start, loc, k, False) # Create agent object and put it in the dictionary\n",
        "      map[loc[0],loc[1]] = k                              # Record it on the map by its team number\n",
        "      agent_dict[id].memory.wipe()                        # Reset memory\n",
        "      \n",
        "  return map, agent_dict, team_scores#, experience_replays\n",
        "\n",
        "# Reset map and team scores, preserve agents\n",
        "def reset(agent_dict):\n",
        "  map = -np.ones([M,M],dtype=np.int)         # M x M map\n",
        "  team_scores = np.zeros(K, dtype=np.int)\n",
        "  #experience_replays = []                    # Experience replays (for Q-learning)\n",
        "  \n",
        "  for k in range(1, K+1):   # Loop through teams\n",
        "    for n in range(N):      # Loop through agents per team\n",
        "      agnt = agent_dict[N*(k-1) + n] # Take agent\n",
        "      \n",
        "      loc = np.array([int(M/K)*(k-1),int(M/N)*n])       # Spawn location\n",
        "      agnt.loc = loc     # Respawn\n",
        "      agnt.hp = hp_start # Revive\n",
        "      map[loc[0],loc[1]] = agnt.team    # Record on the map\n",
        "            \n",
        "  return map, team_scores#, experience_replays\n",
        "\n",
        "\n",
        "# Return queue of surviving agents (the order in which they will act)\n",
        "def action_queue(agent_dict):\n",
        "  queue = []\n",
        "\n",
        "  for ind, agnt in agent_dict.items(): # Loop through all agents\n",
        "    if agnt.alive():                   # Only include the living\n",
        "      queue.append(ind)\n",
        "\n",
        "  np.random.shuffle(queue)               # Random shuffle\n",
        "  \n",
        "  return queue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pJ1bidU-Rbnj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ]
    },
    {
      "metadata": {
        "id": "9Two2wZWRewZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_map(map):\n",
        "  # Set background\n",
        "  map_plot = copy.deepcopy(map)\n",
        "  map_plot[map_plot == -1] = 8\n",
        "  \n",
        "  # Show map\n",
        "  plt.imshow(map_plot, cmap='Set1',interpolation='none',aspect='equal')\n",
        "\n",
        "  # Current axis\n",
        "  ax = plt.gca()\n",
        "\n",
        "  # Set up grid\n",
        "  ax.set_xticks(np.arange(-0.5,M,1));\n",
        "  ax.set_yticks(np.arange(-0.5,M,1));\n",
        "\n",
        "  ax.set_xticklabels([]);\n",
        "  ax.set_yticklabels([]);\n",
        "\n",
        "  # Show grid\n",
        "  plt.grid(color='w')\n",
        "  \n",
        "  # Display\n",
        "  display.display(plt.gcf())\n",
        "  display.clear_output(wait=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "09Uf2fm1RZfM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stats"
      ]
    },
    {
      "metadata": {
        "id": "AIHX5P3_RSt0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Returns a Dictionary of Agent IDs and their HPs\n",
        "def hp_dict():    \n",
        "  hps = {}\n",
        "  for ind, agnt in agent_dict.items(): # Loop through all agents\n",
        "    hps[ind] = agnt.hp # Record hp\n",
        "  return hps\n",
        "\n",
        "# Returns a Dictionary of Agent IDs and their Reward Scores\n",
        "def reward_dict():\n",
        "  rewards = {}\n",
        "  for ind, agnt in agent_dict.items(): # Loop through all agents\n",
        "    rewards[ind] = agnt.personal_reward # Record personal reward\n",
        "  return rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RHozNaDCOBz8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Initialize"
      ]
    },
    {
      "metadata": {
        "id": "FU_UFRbR6T_T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "7847a69e-d022-4cab-aacb-0e55692dbb02"
      },
      "cell_type": "code",
      "source": [
        "# map = M x M grid where each site has -1 for empty space or the team number of the occupying agent\n",
        "# agent_dict = dictionary of all agents (initialized, then untouched); \n",
        "\n",
        "map, agent_dict, team_scores = initialize()\n",
        "plot_map(map)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAABQlJREFUeJzt2zFunVUQhuE/CCkSC6CxULbBDlJE\ngiwgFYWzDBSxClKkYgEEkYIdZBso8iJSmYbTHWJxfeXz+p7nKT2KNaPrTxmfSZ7c3t4eQM9XqxsA\n5oQTooQTooQTooQTor7+UvH28+fbJ0+fPlQvsKsn0y/ecUq5vbn6blq4uvl0vH37dlq7vr7+37VT\n/syptRdvfjmubj4ds9nOPdeptft8v3N+ZqWZL/Fn8d/aNJzWWogSTogSTogSTogSToi687X2oRqB\njU1fa7945zyOI/OMro/L72PHmUdtxloLUcIJUcIJUcIJUcIJUU4psJ5Tij7afew486jNWGshSjgh\nSjghSjghSjghyikF1nNK0Ue7jx1nHrUZay1ECSdECSdECSdECSdEOaXAek4p+mj3sePMozZjrYUo\n4YQo4YQo4YQor7WwntdafbT72HHmUZux1kKUcEKUcEKUcEKUcEKUUwqs55Sij3YfO848ajPWWogS\nTogSTogSTogSTohySoH1nFL00e5jx5lHbcZaC1HCCVHCCVHCCVF3Pgi9u3k2/fr8V9jH4d3Ns+P6\nmM/2mOcaLvEzO47Lneu/OKXAeqedUr7/+a/p1z++ef5on6/f3Tw7Pr55Pp3t3HOdWrvP9zvnZ1aa\n+RJ/Fkdtxu+cECWcECWcECWcEHXng9BPV38/RB8Pasx0ibMdh7kuhVMKrOd/peij3ceOM4/ajN85\nIUo4IUo4IUo4IcprLazntVYf7T52nHnUZqy1ECWcECWcECWcECWcEOWUAus5peij3ceOM4/ajLUW\nooQTooQTooQTooQTopxSYD2nFH20+9hx5lGbsdZClHBClHBClHBClHBClFMKrOeUoo92HzvOPGoz\n1lqIEk6IEk6IEk6I8loL63mt1Ue7jx1nHrUZay1ECSdECSdECSdECSdEOaXAek4p+mj3sePMozZj\nrYUo4YQo4YQo4YQo4YQopxRY77RTyg+/v5h+/Y+XHx7t8/Wf374//nj5YTrbuec6tXaf73fOz6w0\n8yX+LI7ajLUWooQTooQTooQTooQTopxSYD3/K0Uf7T52nHnUZqy1ECWcECWcECWcECWcEOWUAus5\npeij3ceOM4/ajLUWooQTooQTooQTorzWwnpea/XR7mPHmUdtxloLUcIJUcIJUcIJUcIJUU4psJ5T\nij7afew486jNWGshSjghSjghSjghSjghyikF1nNK0Ue7jx1nHrUZay1ECSdECSdECSdECSdEOaXA\nek4p+mj3sePMozZjrYUo4YQo4YQo4YQo4YQopxRYzylFH+0+dpx51GastRAlnBAlnBAlnBDltRbW\nO+219tcff5t+/fX7V4/2hez2wzfH6/evprOde65Ta/f5fuf8zEozX+LP4qjNWGshSjghSjghSjgh\nSjghyikF1vMP3/XR7mPHmUdtxloLUcIJUcIJUcIJUcIJUU4psJ5Tij7afew486jNWGshSjghSjgh\nSjghSjghyikF1nNK0Ue7jx1nHrUZay1ECSdECSdECSdECSdEOaXAek4p+mj3sePMozZjrYUo4YQo\n4YQo4YQor7WwntdafbT72HHmUZux1kKUcEKUcEKUcEKUcEKUUwqs55Sij3YfO848ajPWWogSTogS\nTogSTogSTohySoH1nFL00e5jx5lHbcZaC1HCCVHCCVHCCVHCCVF3nVKARfzNCVHCCVHCCVHCCVHC\nCVHCCVH/ABqBpT3PO1afAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f90f49cdc18>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Fi-0X2iXJSCu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Simulation"
      ]
    },
    {
      "metadata": {
        "id": "Jewf2wP3lhE4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pre-populating memory\n",
        "\n",
        "Do we want pretrain length as a variable input? Right now it's just to fill one batch"
      ]
    },
    {
      "metadata": {
        "id": "PsURN-LglgeT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "4e514434-2d94-4a6e-db9d-b1608d976795"
      },
      "cell_type": "code",
      "source": [
        "map, team_scores = reset(agent_dict)\n",
        "\n",
        "# Returns true if all experience replays are at max capacity.\n",
        "def full_mem():\n",
        "  all_clear = True\n",
        "  for ind, agnt in agent_dict.items():\n",
        "    if not agnt.memory.is_full():\n",
        "      all_clear = False\n",
        "  return all_clear\n",
        "    \n",
        "prepop_turn = 1\n",
        "agents_to_go_list = []\n",
        "while prepop_turn < 20000: #not full_mem():\n",
        "  turn_queue = []\n",
        "  for ind, agnt in agent_dict.items():\n",
        "    if agnt.alive() and not agnt.memory.is_full():\n",
        "      turn_queue.append(ind)\n",
        "  np.random.shuffle(turn_queue)\n",
        "  \n",
        "  if prepop_turn % 1000 == 0:\n",
        "    print(\"----Populating memory, turn\", prepop_turn, \"----\")\n",
        "    agents_to_go_list = []\n",
        "    for ind, agnt in agent_dict.items():\n",
        "      if not agnt.memory.is_full():\n",
        "        agents_to_go_list.append(ind)\n",
        "    print(\"Agents to go:\", len(agents_to_go_list))\n",
        "    map, team_scores = reset(agent_dict)\n",
        "\n",
        "  # Reset map if there is one survivor\n",
        "  if len(turn_queue) == 1:\n",
        "    map, team_scores = reset(agent_dict)\n",
        "    continue\n",
        "  \n",
        "  # Loop through alive and non-memory-saturated agents\n",
        "  for j in turn_queue:   \n",
        "    # Agent object\n",
        "    current_agent = agent_dict[j]\n",
        "\n",
        "    # If already dead, pass turn.\n",
        "    if not current_agent.alive():\n",
        "      continue;\n",
        "\n",
        "    # State formation\n",
        "    state = current_agent.observe(map)\n",
        "\n",
        "    # Action selection\n",
        "    action = np.random.choice(4)    \n",
        "    if action == 0:                 # North\n",
        "      dir = np.array([-1,0])\n",
        "    elif action == 1:               # East\n",
        "      dir = np.array([0,1])\n",
        "    elif action == 2:               # West\n",
        "      dir = np.array([0,-1])\n",
        "    elif action  == 3:              # South\n",
        "      dir = np.array([1,0])\n",
        "\n",
        "    # Take the action\n",
        "    personal_point,team_point = current_agent.act(dir,prepop_turn, True)\n",
        "\n",
        "    # New state\n",
        "    state_new = current_agent.observe(map)\n",
        "\n",
        "    # Immediate reward\n",
        "    reward = sigma * personal_point + (1-sigma) * team_point\n",
        "    \n",
        "    # Add experience to memory: [s,a,r,s']\n",
        "    current_agent.memory.add((state, action, reward, state_new))\n",
        "  \n",
        "  prepop_turn += 1\n",
        "\n",
        "print(\"Memory Population took\", prepop_turn, \"turns.\")"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-738b7bae93d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;31m# Reset map if there is one survivor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mturn_queue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteam_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-f427b41f7d9a>\u001b[0m in \u001b[0;36mreset\u001b[0;34m(agent_dict)\u001b[0m\n\u001b[1;32m     51\u001b[0m       \u001b[0magnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloc\u001b[0m     \u001b[0;31m# Respawn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0magnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhp_start\u001b[0m \u001b[0;31m# Revive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m       \u001b[0mmap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteam\u001b[0m    \u001b[0;31m# Record on the map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m       \u001b[0;31m#experience_replays.append([])     # Create experience replay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "va8VHFPBNHmj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "533ce54b-5fdf-46a0-c1f5-469d10c494f6"
      },
      "cell_type": "code",
      "source": [
        "for ind, agnt in agent_dict.items():\n",
        "  print(agnt.memory.len())"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9857\n",
            "10000\n",
            "10000\n",
            "10000\n",
            "10000\n",
            "10000\n",
            "10000\n",
            "10000\n",
            "10000\n",
            "10000\n",
            "10000\n",
            "10000\n",
            "10000\n",
            "10000\n",
            "10000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tzRkV5Z1loDz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training\n",
        "\n",
        "Want to get it working before adding the target network: one step at a time!"
      ]
    },
    {
      "metadata": {
        "id": "kwcWsjvHDzz2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "197f408e-6407-451d-f00d-6e6c56e50335"
      },
      "cell_type": "code",
      "source": [
        "#np.random.seed(1)\n",
        "      \n",
        "\n",
        "# Training\n",
        "sess = tf.InteractiveSession() # Initialize session\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for i_ep in range(n_ep): # Loop through games\n",
        "  \n",
        "  # Reset map and team scores\n",
        "  map, team_scores = reset(agent_dict)\n",
        "  \n",
        "  for turn in range(max_turn):\n",
        "    turn_queue = action_queue(agent_dict) # Order of taking actions this turn\n",
        "\n",
        "    # End of game\n",
        "    if len(turn_queue) == 1:\n",
        "      print(\"David\", turn_queue[0], \"is the lone survivor.\")\n",
        "      print(\"Team Scores:\", team_scores)\n",
        "      print(\"Game\", str(i_ep), \"ended on turn\", turn, \"-----------------------\")\n",
        "      break\n",
        "\n",
        "    # Loop through alive agents\n",
        "    for j in turn_queue:\n",
        "      # Agent object\n",
        "      current_agent = agent_dict[j]\n",
        "      \n",
        "      # If dead in queue, pass turn.\n",
        "      if not current_agent.alive():\n",
        "        continue;\n",
        "\n",
        "      state = current_agent.observe(map)    # State formation\n",
        "      \n",
        "      #action = current_agent.choose_action(state) # Action selection\n",
        "      q_values = sess.run(current_agent.DQN(state)) # Get q-values from the network\n",
        "      p_values = tf.exp(beta * q_values)/tf.reduce_sum(tf.exp(beta * q_values)) # Boltzmann probabilities\n",
        "      action = np.random.choice(np.arange(4),p=p_values.eval())\n",
        "      \n",
        "      dir = get_dir(action) # Convert to direction\n",
        "\n",
        "      # Take the action\n",
        "      personal_point,team_point = current_agent.act(dir, turn, False)\n",
        "      \n",
        "      # New state\n",
        "      state_new = current_agent.observe(map)\n",
        "\n",
        "      # Immediate reward\n",
        "      reward = sigma * personal_point + (1-sigma) * team_point\n",
        "      \n",
        "      # Add experience to memory: [s,a,r,s']\n",
        "      current_agent.memory.add((state, action, reward, state_new))\n",
        "      \n",
        "      \n",
        "      # Update Q-function\n",
        "      if not current_agent.tenure:\n",
        "        #current_agent.contemplate(experience_replays[j])\n",
        "        batch = current_agent.memory.sample(batch_size)\n",
        "\n",
        "        # Separate out s,a,r,s' from the batch which is a list of lists\n",
        "        states = np.array([item[0] for item in batch])\n",
        "        actions = np.array([item[1] for item in batch])\n",
        "        rewards = np.array([item[2] for item in batch])\n",
        "        next_states = np.array([item[3] for item in batch])\n",
        "        '''\n",
        "        # Reshape (-1 automatically determines the appropriate length)\n",
        "        states = np.reshape(states,[actual_batch_size, -1])\n",
        "        actions = np.reshape(actions,[actual_batch_size, -1])\n",
        "        rewards = np.reshape(rewards,[actual_batch_size, -1])\n",
        "        next_states = np.reshape(next_states,[actual_batch_size, -1])\n",
        "        '''\n",
        "        # Q-value of the next state\n",
        "        Q_next = sess.run(current_agent.DQN.outputs,\n",
        "                          feed_dict = {current_agent.DQN.inputs:next_states})\n",
        "\n",
        "        # Target\n",
        "        target_Q_batch = []\n",
        "        for i in len(batch):\n",
        "          target = rewards[i] + gamma * Q_next[i] #Do we want to apply np.max() to Q_next?\n",
        "          target_Q_batch.append(target)\n",
        "        # Not sure if this is necessary, I think it just converts to np.array format?\n",
        "        targets_Qs = np.array([each for each in target_Q_batch])\n",
        "\n",
        "        loss, _ = sess.run([current_agent.DQN.loss, current_agent.DQN.optimizer],\n",
        "                            feed_dict={current_agent.DQN.inputs: states,\n",
        "                                       current_agent.DQN.target_Q: target_Qs,\n",
        "                                       current_agent.DQN.actions: actions})\n",
        "        \n",
        "        '''\n",
        "        # Write TF Summaries\n",
        "        summary = sess.run(write_op, feed_dict={current_agent.DQN.inputs: states,\n",
        "                                           current_agent.DQN.target_Q: target_Qs\n",
        "                                           current_agent.DQN.actions: actions})\n",
        "        writer.add_summary(summary, i_ep)\n",
        "        writer.flush()\n",
        "        '''\n",
        "\n",
        "\n",
        "        '''\n",
        "        TBD\n",
        "        # Update target Q-network every some turns\n",
        "        if (turn+1) % target_copy_freq == 0:\n",
        "          current_agent.DQN_target = copy.deepcopy(current_agent.DQN)\n",
        "        '''\n",
        "    \n",
        "    # Graphics -----------------------------------------------------------------\n",
        "    #print(\"Game \" + str(i_ep) + \", turn \" + str(turn))\n",
        "    #plot_map(map)\n",
        "    #time.sleep(1)\n",
        "    \n",
        "    # Game ended without conclusion\n",
        "    if turn == max_turn-1:\n",
        "      print(\"Game\", str(i_ep), \"ended on turn\", turn, \"-----------------------\")\n",
        "    \n",
        "sess.close() # Close session"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-e6791d6f9916>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0;31m#action = current_agent.choose_action(state) # Action selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Get q-values from the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m       \u001b[0mp_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Boltzmann probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'Net' object is not callable"
          ]
        }
      ]
    }
  ]
}