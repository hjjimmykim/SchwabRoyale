{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SchwabRoyale.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "oIo5MwsnKqOU",
        "9gxqu8GD7wlo",
        "i9BFTN_2_pVC",
        "WOm2e3M393BK",
        "Jewf2wP3lhE4",
        "do24-egggpY_",
        "UudmdRNIgth2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hjjimmykim/SchwabRoyale/blob/master/SchwabRoyale_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "n_BXMiSjENN-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Schwab Royale\n",
        "\n",
        "In the beginning there was darkness.\n",
        "Then there was David."
      ]
    },
    {
      "metadata": {
        "id": "kz_6OvkOItkQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What we have:\n",
        "1. Multi-team multi-agent battle royale setup where each agent gets personal reward for killing anyone and a shared team reward for killing opponent teams.\n",
        "2. Basic Q-learning with experience replay and target Q-networks.\n",
        "\n",
        "Extra features to be implemented: <br>\n",
        "1. Limited visibility (partial information)\n",
        "2. Variants in agent types (e.g. healers, necromancers) with different goals (e.g. save rather than kill things).\n",
        "3. Variance in agent parameters (e.g. selfishness)\n",
        "4. Different game setups (e.g. free-for-all, KotH)\n",
        "5. Team reward per capita -> emergence of spite?\n",
        "6. Communication.\n",
        "\n",
        "Research Ideas: <br>\n",
        "1. Something with mean field game theory?\n",
        "2. Phase transitions in behavior by agent parameters (e.g. selfish psychopath killing its own team members -> defect vs. coop. a la game theory?).\n",
        "3. Emergent language. Different dialects forming within different teams? If you allow them to overhear each other, do they develop encryption/decryption?\n",
        "4. How to deal with sparse rewards? Curiosity-driven learning, options, etc.?\n",
        "5. Active learning (e.g. pick experiences from experience replay where something interesting happens)\n",
        "6. Curriculum learning (e.g. start from easier scenarios like where everyone's next to each other -> learns that killing peope gives you rewards and then move on to harder scenarios where everyone's farther apart -> have to learn to get closer to each other to reduce the problem to what was already learned)\n",
        "7. Metacontroller (\"Overmind\") per each team that tunes each team member's hyperparameters."
      ]
    },
    {
      "metadata": {
        "id": "mr1q4B6P18vs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ]
    },
    {
      "metadata": {
        "id": "N5jfqjoT2OrU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "8f27b699-0f0c-456b-f0d6-01f725a2e25d"
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display # For animation\n",
        "\n",
        "import time # Keeping track of time\n",
        "import copy\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "# Machine learning\n",
        "import tensorflow as tf"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[autoreload of PIL.Image failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'py3'\n",
            "]\n",
            "[autoreload of PIL._binary failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'py3'\n",
            "]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "IH8FIvLJHeDW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ]
    },
    {
      "metadata": {
        "id": "53uGs_23HfR_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Agent parameters\n",
        "K = 2           # Number of teams\n",
        "N = 2           # Number of agents per team\n",
        "hp_start = 2    # Starting hp\n",
        "action_size = 4 # Four cardinal directions\n",
        "\n",
        "# Simulation parameters\n",
        "M = 8    # Width of map\n",
        "max_turn = 10000 # Maximum number of turns per episode\n",
        "record_turn = int(max_turn/100) # Record time every record_turn turns\n",
        "n_ep = 2  # Number of training episodes (games)\n",
        "\n",
        "# Reinforcement learning parameters\n",
        "batch_size = 10 # Batch size\n",
        "memory_size = 100 # Number of experiences agent can keep\n",
        "hidden_dim = 10 # Hidden layer size\n",
        "\n",
        "target_copy_freq = 10 # Update the target network every this many turns\n",
        "\n",
        "alpha = 0.01 # Learning rate\n",
        "beta = 0.1   # Exploitation parameter\n",
        "gamma = 0.9  # Discount factor\n",
        "sigma = 0.5  # Selfishness factor\n",
        "Omega = N    # Final survivor bonus (should it be for team and/or self?)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HDXNiVV_ssWR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Spunk\n",
        "\n",
        "Constantijn takes responsibility for the time wasted making this, however, Jimmy supported him."
      ]
    },
    {
      "metadata": {
        "id": "CeExgoRAspoP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "death_messages = [\"was wasted by\",\n",
        "                  \"met a bitter end at the hands of\",\n",
        "                  \"was swarmed by TensorBros sent by\",\n",
        "                  \"was forced to press Alt+F4 by\",\n",
        "                  \"was peer-reviewed by\",\n",
        "                  \"got schwabbed by\", # https://www.urbandictionary.com/define.php?term=Schwabbed\n",
        "                  \"was brutally neglected by\",\n",
        "                  \"had his heart stolen by\",\n",
        "                  \"slipped and fell while running from\",\n",
        "                  \"became petrified by the sight of\",\n",
        "                  \"wished Asimov protected him against\",\n",
        "                  \"was denied cluster access by\"\n",
        "                 ]\n",
        "\n",
        "def kill_log(killer, killer_team, victim, victim_team, turn):\n",
        "  phrase = \"%s [Team %i] \" + np.random.choice(death_messages) + \" %s [Team %i] on turn %i.\"\n",
        "  return phrase % (\"David \"+str(victim), victim_team, \"David \"+str(killer), killer_team, turn)\n",
        "\n",
        "def kill_log_straw(killer, killer_team, victim, victim_team, turn):\n",
        "  phrase = \"%s [Team %i] \" + np.random.choice(death_messages) + \" %s [Team %i] on turn %i.\"\n",
        "  return phrase % (\"Straw Man \"+str(victim), victim_team, \"Straw Man \"+str(killer), killer_team, turn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U_l6IAIO-hd4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Network"
      ]
    },
    {
      "metadata": {
        "id": "VesLnp7DMAOB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "*   https://web.stanford.edu/class/cs20si/2017/lectures/slides_14.pdf\n",
        "*   https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Deep%20Q%20Learning/Doom/Deep%20Q%20learning%20with%20Doom.ipynb\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "PV0aOsNe-kQh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Net:\n",
        "  def __init__(self, input_dim, hidden_dim = 100, output_dim = 4, name=\"brain_name\"):\n",
        "    # Fully-connected feedforward network with 2 hidden layers    \n",
        "    # input-dim = 1-D reshaped map + hp\n",
        "    # hidden_dim = number of units per hidden layer\n",
        "    # output_dim = number of outputs (i.e. actions)\n",
        "\n",
        "    # Input placeholder\n",
        "    self.inputs = tf.placeholder(tf.float32, shape=[None,input_dim], name=\"inputs\")\n",
        "\n",
        "    ## Weight initialization\n",
        "    # input -> h1\n",
        "    W0 = tf.Variable(tf.truncated_normal([input_dim,hidden_dim],stddev=0.1), name=name)\n",
        "    b0 = tf.Variable(tf.constant(0.1,shape=[hidden_dim]), name=name)\n",
        "\n",
        "    # h1 -> h2\n",
        "    W1 = tf.Variable(tf.truncated_normal([hidden_dim,hidden_dim],stddev=0.1), name=name)\n",
        "    b1 = tf.Variable(tf.constant(0.1,shape=[hidden_dim]), name=name)\n",
        "    \n",
        "    # h2 -> output\n",
        "    W2 = tf.Variable(tf.truncated_normal([hidden_dim,output_dim],stddev=0.1), name=name)\n",
        "    b2 = tf.Variable(tf.constant(0.1,shape=[output_dim]), name=name)\n",
        "    \n",
        "    \n",
        "    ## Hidden layers\n",
        "    h1 = tf.nn.relu(tf.matmul(self.inputs,W0) + b0)\n",
        "    h2 = tf.nn.relu(tf.matmul(h1,W1) + b1)\n",
        "    \n",
        "    ## Output layer\n",
        "    self.outputs = tf.matmul(h2,W2) + b2\n",
        "    \n",
        "    ## Keep track of parameters\n",
        "    self.params = [W0,b0,W1,b1,W2,b2]\n",
        "    \n",
        "    \n",
        "    #Learnability\n",
        "    self.actions = tf.placeholder(tf.int32, shape=[None, 1], name=\"actions\")\n",
        "    idx = tf.stack([tf.range(tf.shape(self.actions)[0]),self.actions[:,0]],axis=1) # Indices to gather; of form [[0,actions[0]],[1,actions[1]], ...]\n",
        "    \n",
        "    self.Q = tf.gather_nd(self.outputs,idx) # Pick out q-values corresponding to selected actions\n",
        "\n",
        "    self.target_Q = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "    \n",
        "    self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))    \n",
        "    self.optimizer = tf.train.AdamOptimizer(alpha).minimize(self.loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SyLYUKe-JNTL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Memory Class source: https://github.com/udacity/deep-learning/blob/master/reinforcement/Q-learning-cart.ipynb"
      ]
    },
    {
      "metadata": {
        "id": "dI-HynsoJMgH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Memory:\n",
        "  def __init__(self, max_size):\n",
        "    self.max = max_size\n",
        "    self.buffer = deque(maxlen = max_size)    # \"deque\" = Double Ended QUEue\n",
        "  \n",
        "  # Add entry. FIFO.\n",
        "  def add(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "  \n",
        "  # Sample batch_size number of entries, without replacement. Return as a list.\n",
        "  def sample(self, batch_size):\n",
        "    buffer_size = len(self.buffer)\n",
        "\n",
        "    index = np.random.choice(np.arange(buffer_size),\n",
        "                             size = min(batch_size,buffer_size),\n",
        "                             replace = False)\n",
        "    return [self.buffer[i] for i in index]\n",
        "  \n",
        "  # Own additions\n",
        "  def is_full(self):\n",
        "    return len(self.buffer) == self.max\n",
        "  \n",
        "  def wipe(self):\n",
        "    self.buffer.clear() # NB, does not affect max length\n",
        "\n",
        "  def length(self):\n",
        "    return len(self.buffer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oIo5MwsnKqOU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Stanford fancy memory alternative:"
      ]
    },
    {
      "metadata": {
        "id": "tGZ4T3gLLqqY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "  \n",
        "  def __init__(self, template, capacity):\n",
        "    self._capacity = capacity # Number of memories to maintain\n",
        "    self._buffers = self._create_buffers(template)\n",
        "    self._index = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
        "    \n",
        "  # Number of memories saved\n",
        "  def size(self):\n",
        "    return tf.minimum(self._index, self._capacity)\n",
        "  \n",
        "  def append(self, tensors):\n",
        "    position = tf.mod(self._index, self._capacity)\n",
        "    with tf.control_dependencies([\n",
        "        b[position].assign(t) for b, t in\n",
        "        zip(self._buffers, tensors)]):\n",
        "      return self._index.assign_add(1)\n",
        "  \n",
        "  def sample(self, amount):\n",
        "    positions = tf.random_uniform((amount,), 0, self.size - 1, tf.int32)\n",
        "    return [tf.gather(b, positions)\n",
        "            for b in self._buffers]\n",
        "\n",
        "  def _create_buffers(self, template):\n",
        "    buffers = []\n",
        "    for tensor in template:\n",
        "      shape = tf.TensorShape([self._capacity]).concatenate(\n",
        "          tensor.get_shape())\n",
        "      initial = tf.zeros(shape, tensor.dtype)\n",
        "      buffers.append(tf.Variable(\n",
        "          initial, trainable=False))\n",
        "    return buffers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lmn1TntO3jey",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Agent"
      ]
    },
    {
      "metadata": {
        "id": "8B0tnXDx3d7o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "  def __init__(self, id, hp, loc, k, memory_size, tenure):\n",
        "    self.id = id    # Agent id (in agent_dict)\n",
        "    self.hp = hp    # Hit points\n",
        "    self.loc = loc  # Location (r,c coordinate [r,c])\n",
        "    self.team = k   # Team index (k = 1, ..., K), since 0 is self!\n",
        "    self.memory_size = memory_size # Experience replay maximum capacity\n",
        "    self.tenure = tenure       # If you don't have tenure yet, you gotta learn (i.e. determines whether the agent is learning)\n",
        "    \n",
        "    self.personal_reward = 0   # Keep track of personal reward    \n",
        "    \n",
        "    # Create brain\n",
        "    input_dim = M*M + 1 # Map cells + hp\n",
        "    output_dim = action_size\n",
        "    \n",
        "    self.DQN = Net(input_dim, hidden_dim, output_dim, name=\"Q_primary\"+str(id)) # Personal neural network\n",
        "    self.DQN_target = Net(input_dim, hidden_dim, output_dim, name=\"Q_target\"+str(id))         # Target network\n",
        "    \n",
        "    self.memory = Memory(memory_size) # Experience replay\n",
        "    \n",
        "  \n",
        "  # Checks living status\n",
        "  def alive(self):\n",
        "    return self.hp > 0\n",
        "\n",
        "  # State formation\n",
        "  def observe(self, map):\n",
        "    state = copy.deepcopy(map)\n",
        "    state[self.loc[0]][self.loc[1]] = 0            # Own location = 0 on map\n",
        "    state = np.reshape(state, [1,-1]).squeeze()    # Convert to 1D array\n",
        "    state = np.hstack([state, self.hp])            # Append hp\n",
        "    return state\n",
        "  '''\n",
        "  # Takes a state, and returns an action from the action space\n",
        "  def choose_action(self, state, sess):\n",
        "    # Action selection (Q function) ------------------------------------------\n",
        "    q_values = self.DQN(state) # Get q-values from the network\n",
        "    p_values = tf.exp(beta * q_values)/tf.reduce_sum(tf.exp(beta * q_values)) # Boltzmann probabilities\n",
        "    \n",
        "    action = np.random.choice(np.arange(4),p=p_values.eval())\n",
        "    return action\n",
        "  '''\n",
        "  \n",
        "  # Taking an action based on chosen direction\n",
        "  def act(self, dir, turn, quiet):\n",
        "    # dir should be an np.array\n",
        "    # turn = just for displaying kill message\n",
        "    # quiet = print kill message or not\n",
        "    \n",
        "    # Rewards resulting from this move\n",
        "    personal_point = 0\n",
        "    team_point = 0\n",
        "    \n",
        "    target_loc = self.loc + dir # Candidate target location\n",
        "    \n",
        "    # Check if target location is within bounds (make sure the agent cannot move into itself)\n",
        "    if check_valid_loc(target_loc):\n",
        "      \n",
        "      target_ind = map[target_loc[0]][target_loc[1]]    # Object at target location\n",
        "      \n",
        "      if target_ind == -1:                              # If target location is empty\n",
        "        map[self.loc[0],self.loc[1]] = -1               # Previous location becomes empty\n",
        "        map[target_loc[0],target_loc[1]] = self.team    # Target location becomes occupied\n",
        "        self.loc = target_loc                           # Update location\n",
        "      else:                                             # If target location is occupied\n",
        "        # Agent at target location\n",
        "        target_id = find_agent(target_loc)\n",
        "        target_agent = agent_dict[target_id]\n",
        "        '''\n",
        "        if not target_agent.alive(): # Target agent is already dead; this should not run\n",
        "          print(\"David\", self.id, \"teabagged David\", target_id)\n",
        "        '''\n",
        "        target_agent.hp -= 1                            # Deal damage\n",
        "\n",
        "        if not target_agent.alive():                    # If target agent has been killed\n",
        "          # Display kill log\n",
        "          if not quiet:\n",
        "            print(kill_log(self.id, self.team, target_id, target_agent.team, turn))\n",
        "\n",
        "          target_agent.loc = [-1,-1]                    # Move corpse to the underworld\n",
        "\n",
        "          map[self.loc[0],self.loc[1]] = -1             # Previous location becomes empty\n",
        "          map[target_loc[0],target_loc[1]] = self.team  # Target location becomes occupied\n",
        "          self.loc = target_loc                         # Update location\n",
        "          \n",
        "          personal_point += 1             # Gain a point for self\n",
        "          if target_agent.team != self.team:\n",
        "            team_point += 1               # Gain a point for the team, if the target was from another team\n",
        "  \n",
        "    # Update the cumulative rewards\n",
        "    self.personal_reward += personal_point\n",
        "    team_scores[self.team-1] += team_point\n",
        "    \n",
        "    # Return immediate rewards\n",
        "    return personal_point, team_point"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JA1jrpr3iS_0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Straw_Man:\n",
        "  def __init__(self, id, hp, loc, k):\n",
        "    self.id = id    # Agent id (in agent_dict)\n",
        "    self.hp = hp    # Hit points\n",
        "    self.loc = loc  # Location (r,c coordinate [r,c])\n",
        "    self.team = k   # Team index (k = 1, ..., K), since 0 is self!\n",
        "    \n",
        "    self.p_values = []\n",
        "    self.p_values = np.exp(beta * np.random.normal(0,1, [1,4]))\n",
        "    self.p_values = self.p_values/np.sum(self.p_values)\n",
        "    \n",
        "  \n",
        "  # Checks living status\n",
        "  def alive(self):\n",
        "    return self.hp > 0\n",
        "\n",
        "  \n",
        "  # Taking an action based on chosen direction\n",
        "  def Boltz_act(self):\n",
        "    \n",
        "    action = np.random.choice(np.arange(4),p=np.squeeze(self.p_values))\n",
        "    \n",
        "    target_loc = self.loc + get_dir(action) # Candidate target location\n",
        "    \n",
        "    # Check if target location is within bounds (make sure the agent cannot move into itself)\n",
        "    if check_valid_loc(target_loc):\n",
        "      \n",
        "      target_ind = map[target_loc[0]][target_loc[1]]    # Object at target location\n",
        "      \n",
        "      if target_ind == -1:                              # If target location is empty\n",
        "        map[self.loc[0],self.loc[1]] = -1               # Previous location becomes empty\n",
        "        map[target_loc[0],target_loc[1]] = self.team    # Target location becomes occupied\n",
        "        self.loc = target_loc                           # Update location\n",
        "      else:                                             # If target location is occupied\n",
        "        # Agent at target location\n",
        "        target_id = find_agent(target_loc)\n",
        "        target_agent = agent_dict[target_id]\n",
        "        \n",
        "        target_agent.hp -= 1                            # Deal damage\n",
        "\n",
        "        if not target_agent.alive():                    # If target agent has been killed\n",
        "          # Display kill log\n",
        "          print(kill_log_straw(self.id, self.team, target_id, target_agent.team, turn))\n",
        "\n",
        "          target_agent.loc = [-1,-1]                    # Move corpse to the underworld\n",
        "\n",
        "          map[self.loc[0],self.loc[1]] = -1             # Previous location becomes empty\n",
        "          map[target_loc[0],target_loc[1]] = self.team  # Target location becomes occupied\n",
        "          self.loc = target_loc                         # Update location"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9gxqu8GD7wlo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Agent Testing"
      ]
    },
    {
      "metadata": {
        "id": "rAXGZJB3mryV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "N.B.: Before testing agent features, run Initialize below"
      ]
    },
    {
      "metadata": {
        "id": "i9BFTN_2_pVC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Movement Testing\n",
        "\n",
        "Test whether movement is recorded on the map properly, \n",
        "test agent's functions"
      ]
    },
    {
      "metadata": {
        "id": "HezoyymCjO8I",
        "colab_type": "code",
        "outputId": "67caf2d0-ff17-466d-a349-21c5e94939a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "cell_type": "code",
      "source": [
        "initialize_map()\n",
        "test_agent = agent_dict[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-98eb3a6b9ae9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minitialize_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'initialize_map' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "U-9rGq367AZH",
        "colab_type": "code",
        "outputId": "12193ca4-2e7b-4266-eba0-33d025287f2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "agent_dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: <__main__.Agent at 0x7f42fdccefd0>,\n",
              " 1: <__main__.Agent at 0x7f42fdcce668>,\n",
              " 2: <__main__.Agent at 0x7f42fdc90940>,\n",
              " 3: <__main__.Agent at 0x7f42fdc905c0>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 346
        }
      ]
    },
    {
      "metadata": {
        "id": "1Iu1y-TjjoTU",
        "colab_type": "code",
        "outputId": "2a83be2a-8292-4be7-ee2c-2e95257bea1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "print(test_agent.loc)\n",
        "test_agent.observe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0, -1,  1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [ 2, -1,  2, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 347
        }
      ]
    },
    {
      "metadata": {
        "id": "ISh3mw8lkJdh",
        "colab_type": "code",
        "outputId": "7ebe79af-6e57-457d-982e-f99e732a8386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "test_agent.move([1,1])\n",
        "print(test_agent.loc)\n",
        "test_agent.observe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1, -1,  1, -1, -1],\n",
              "       [-1,  0, -1, -1, -1],\n",
              "       [ 2, -1,  2, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 348
        }
      ]
    },
    {
      "metadata": {
        "id": "ojTVFGTo6Dod",
        "colab_type": "code",
        "outputId": "f28872c7-fbf2-4ceb-97e2-c6362cae84a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "test_agent.move([0,-1])\n",
        "print(test_agent.loc)\n",
        "test_agent.observe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1, -1,  1, -1, -1],\n",
              "       [ 0, -1, -1, -1, -1],\n",
              "       [ 2, -1,  2, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 349
        }
      ]
    },
    {
      "metadata": {
        "id": "cH_hnwXP6I3K",
        "colab_type": "code",
        "outputId": "0f0cdada-a2ab-446c-d6f0-7352094ad22f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "test_agent.move([-1,0])\n",
        "print(test_agent.loc)\n",
        "test_agent.observe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0, -1,  1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [ 2, -1,  2, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 350
        }
      ]
    },
    {
      "metadata": {
        "id": "Z6_2HK0EthxI",
        "colab_type": "code",
        "outputId": "8d26b993-714c-4a5e-8a72-b8fc1d669705",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "find_agent([0,0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 351
        }
      ]
    },
    {
      "metadata": {
        "id": "WOm2e3M393BK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Combat Testing\n",
        "\n",
        "Test attacking (movement upon kill), HP tracking, death of a teammate (personal reward), and death of an enemy (personal and team reward)"
      ]
    },
    {
      "metadata": {
        "id": "_oIwAM9E9-1C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "initialize_map()\n",
        "attack_agent = agent_dict[0]\n",
        "gimp_agent = agent_dict[1]\n",
        "enemy_agent = agent_dict[3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sYW8cZbA-Jni",
        "colab_type": "code",
        "outputId": "88ed7a1b-f2dd-4902-eedd-65e4889dd733",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Attacker's HP:\", attack_agent.hp)\n",
        "print(\"Gimp's HP:\", gimp_agent.hp)\n",
        "print(\"Attacker's Score:\", attack_agent.reward)    # Just to check it's well-behaved\n",
        "print(\"Team Scores\", team_scores)\n",
        "attack_agent.observe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attacker's HP: 2\n",
            "Gimp's HP: 2\n",
            "Attacker's Score: 0\n",
            "Team Scores [0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0, -1,  1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [ 2, -1,  2, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 434
        }
      ]
    },
    {
      "metadata": {
        "id": "QU7XuuVt-YOr",
        "colab_type": "code",
        "outputId": "02e57987-53ce-4cc1-bcd4-2b799f73d642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "attack_agent.move([0,2])\n",
        "print(\"Gimp's HP:\", gimp_agent.hp)\n",
        "print(\"Attacker's Score:\", attack_agent.reward)\n",
        "print(\"Team Scores\", team_scores)\n",
        "attack_agent.observe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gimp's HP: 1\n",
            "Attacker's Score: 0\n",
            "Team Scores [0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0, -1,  1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [ 2, -1,  2, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 435
        }
      ]
    },
    {
      "metadata": {
        "id": "v28ctonPApve",
        "colab_type": "code",
        "outputId": "8dcde36f-35db-4d6f-f64b-f195ecdbb453",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "attack_agent.move([0,2])\n",
        "print(\"Gimp's HP:\", gimp_agent.hp)\n",
        "print(\"Attacker's Score:\", attack_agent.reward)\n",
        "print(\"Team Scores\", team_scores)\n",
        "attack_agent.observe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gimp's HP: 0\n",
            "Attacker's Score: 1\n",
            "Team Scores [0 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1, -1,  0, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [ 2, -1,  2, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 436
        }
      ]
    },
    {
      "metadata": {
        "id": "66btVJboCzed",
        "colab_type": "code",
        "outputId": "472fb169-39bf-4393-9b9e-24e128015c80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "attack_agent.move([2,0])\n",
        "attack_agent.move([2,0])\n",
        "print(\"Enemy's HP:\", enemy_agent.hp)\n",
        "print(\"Attacker's Score:\", attack_agent.reward)\n",
        "print(\"Team Scores\", team_scores)\n",
        "attack_agent.observe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enemy's HP: 0\n",
            "Attacker's Score: 2\n",
            "Team Scores [1 0]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1, -1, -1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [ 2, -1,  0, -1, -1],\n",
              "       [-1, -1, -1, -1, -1],\n",
              "       [-1, -1, -1, -1, -1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 437
        }
      ]
    },
    {
      "metadata": {
        "id": "hNcirfSL_MpA",
        "colab_type": "code",
        "outputId": "b5f6db64-7ceb-492a-d0c4-103a4182fc3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "agent_dict"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: <__main__.Agent at 0x7f42fdc64630>, 2: <__main__.Agent at 0x7f42fdc645c0>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 438
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "M8oGCzf5I-Bz",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M0J2q94t42RL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ]
    },
    {
      "metadata": {
        "id": "7LrfjkLfJKKG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Schwifty Functions\n",
        "# Check if the given location is within bounds\n",
        "def check_valid_loc(loc):\n",
        "  return (np.all(0 <= loc) and np.all(loc < M))\n",
        "\n",
        "# Find the agent at a given location\n",
        "def find_agent(loc):\n",
        "  for ind, agnt in agent_dict.items(): # Loop through all agents\n",
        "    if np.all(agnt.loc == loc): # Return the index of agent at the location\n",
        "      return ind\n",
        "    \n",
        "# Translates action space to a direction\n",
        "def get_dir(action):\n",
        "  # NESW\n",
        "  dir_list = [np.array([-1,0]),np.array([0,1]),np.array([1,0]),np.array([0,-1])]\n",
        "  return dir_list[action]\n",
        "\n",
        "# Initialize map, agents, and team scores\n",
        "def initialize():\n",
        "  agent_dict = {}                            # Dictionary of all agents\n",
        "  map = -np.ones([M,M],dtype=np.int)         # M x M map\n",
        "  team_scores = np.zeros(K, dtype=np.int)    # Team scores\n",
        "  \n",
        "  for k in range(1, K+1):    # Loop through teams\n",
        "    for n in range(N):       # Loop through agents per team\n",
        "      id = N*(k-1) + n                                   # Agent id\n",
        "      loc = np.array([int(M/K)*(k-1),int(M/N)*n])        # Starting location\n",
        "  \n",
        "      agent_dict[id] = Agent(id, hp_start, loc, k, memory_size, False) # Create agent object and put it in the dictionary\n",
        "      map[loc[0],loc[1]] = k                              # Record it on the map by its team number\n",
        "      agent_dict[id].memory.wipe()                        # Reset memory\n",
        "      \n",
        "  return map, agent_dict, team_scores\n",
        "\n",
        "# Reset map and team scores, preserve agents\n",
        "def reset(agent_dict):\n",
        "  map = -np.ones([M,M],dtype=np.int)         # M x M map\n",
        "  team_scores = np.zeros(K, dtype=np.int)\n",
        "\n",
        "  for k in range(1, K+1):   # Loop through teams\n",
        "    for n in range(N):      # Loop through agents per team\n",
        "      agnt = agent_dict[N*(k-1) + n] # Take agent\n",
        "      \n",
        "      loc = np.array([int(M/K)*(k-1),int(M/N)*n])       # Spawn location\n",
        "      agnt.loc = loc     # Respawn\n",
        "      agnt.hp = hp_start # Revive\n",
        "      map[loc[0],loc[1]] = agnt.team    # Record on the map\n",
        "            \n",
        "  return map, team_scores\n",
        "\n",
        "\n",
        "# Return queue of surviving agents (the order in which they will act)\n",
        "def action_queue(agent_dict):\n",
        "  queue = []\n",
        "\n",
        "  for ind, agnt in agent_dict.items(): # Loop through all agents\n",
        "    if agnt.alive():                   # Only include the living\n",
        "      queue.append(ind)\n",
        "\n",
        "  np.random.shuffle(queue)               # Random shuffle\n",
        "  \n",
        "  return queue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pJ1bidU-Rbnj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ]
    },
    {
      "metadata": {
        "id": "9Two2wZWRewZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_map(map):\n",
        "  # Set background\n",
        "  map_plot = copy.deepcopy(map)\n",
        "  map_plot[map_plot == -1] = 8 # Color the background\n",
        "  \n",
        "  # Show map\n",
        "  plt.imshow(map_plot, cmap='Set1',interpolation='none',aspect='equal')\n",
        "\n",
        "  # Current axis\n",
        "  ax = plt.gca()\n",
        "\n",
        "  # Set up grid\n",
        "  ax.set_xticks(np.arange(-0.5,M,1));\n",
        "  ax.set_yticks(np.arange(-0.5,M,1));\n",
        "\n",
        "  ax.set_xticklabels([]);\n",
        "  ax.set_yticklabels([]);\n",
        "\n",
        "  # Show grid\n",
        "  plt.grid(color='w')\n",
        "  \n",
        "  # Display\n",
        "  display.display(plt.gcf())\n",
        "  display.clear_output(wait=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "09Uf2fm1RZfM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stats"
      ]
    },
    {
      "metadata": {
        "id": "AIHX5P3_RSt0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Returns a Dictionary of Agent IDs and their HPs\n",
        "def hp_dict():\n",
        "  hps = {}\n",
        "  for in, agnt in agent_dict.items():\n",
        "    hps[ind] = agnt.hp\n",
        "  return hps\n",
        "\n",
        "# Returns a Dictionary of Agent IDs and their HPs\n",
        "def reward_dict():\n",
        "  rewards = {}\n",
        "  for in, agnt in agent_dict.items():\n",
        "    rewards[ind] = agnt.personal_reward\n",
        "  return rewards"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RHozNaDCOBz8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Initialize"
      ]
    },
    {
      "metadata": {
        "id": "FU_UFRbR6T_T",
        "colab_type": "code",
        "outputId": "c6248fb0-0370-442d-f1e0-6229d4140719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "cell_type": "code",
      "source": [
        "# map = M x M grid where each site has -1 for empty space or the team number of the occupying agent\n",
        "# agent_dict = dictionary of all agents (initialized, then untouched); \n",
        "\n",
        "tf.reset_default_graph() # Reset variables\n",
        "map, agent_dict, team_scores = initialize()\n",
        "plot_map(map)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAA4pJREFUeJzt3cFJHGEch+HZIFiEpI90kE62iyBp\nY/vIwQ7SR7AIT+MlewkfDoiZ73X2eY7m8BtZXhwR8j+t67oAPV9mPwAwJk6IEidEiROixAlRd2/9\n4/rysp7u7/d6FrhVp+EXN/6Usj4/fP0/j/OPh+c/y+Vy2WVrWZblfD7vtrf31hE/s6N+Xn/3hnF6\nrYUocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQ\nJU6IEidEiROixAlR4oSoN88xLMuy/Hr8scdzLOddVm6Dz+wYNs8x7PUgcMOG5xg2f3Ie+D7FIb83\nW59r67o34ndOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogS\nJ0SJE6LECVHihChxQpQ4IUqcECVOiHKOAeZzjmHWni1bW3sjXmshSpwQJU6IEidEiROixAlR4oQo\ncUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghyjkGmM85\nhll7tmxt7Y14rYUocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKE\nKHFClDghSpwQJU6IEidEiROixAlR4oQo5xhgvvedY/j2+PTxjzLw++d35xg+aOuIn9lRP6/r3ojX\nWogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR\n4oQocUKUOCFKnBAlTohyjgHme985hiP/F/hH/N5sfa6t696I11qIEidEiROixAlR4oQocUKUOCFK\nnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6Ico4B5nOO\nYdaeLVtbeyNeayFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDgh\nSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBDlVgrM51bKrD1btrb2RrzWQpQ4IUqcECVOiBInRIkT\nosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQtTW\nOQZgEj85IUqcECVOiBInRIkTosQJUa+Zf+/nZBTpDQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f79e3b4ce48>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Fi-0X2iXJSCu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Simulation"
      ]
    },
    {
      "metadata": {
        "id": "Jewf2wP3lhE4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pre-populating memory\n",
        "\n",
        "Do we want pretrain length as a variable input? Right now it's just to fill one batch"
      ]
    },
    {
      "metadata": {
        "id": "PsURN-LglgeT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "map, team_scores = reset(agent_dict)\n",
        "\n",
        "# Returns true if all experience replays are at max capacity.\n",
        "def full_mem():\n",
        "  all_clear = True\n",
        "  for ind, agnt in agent_dict.items():\n",
        "    if not agnt.memory.is_full():\n",
        "      all_clear = False\n",
        "  return all_clear\n",
        "    \n",
        "prepop_turn = 1\n",
        "agents_to_go_list = []\n",
        "while prepop_turn < 15000: #not full_mem():\n",
        "  turn_queue = []\n",
        "  for ind, agnt in agent_dict.items():\n",
        "    if agnt.alive() and not agnt.memory.is_full():\n",
        "      turn_queue.append(ind)\n",
        "  np.random.shuffle(turn_queue)\n",
        "  \n",
        "  if prepop_turn % 1000 == 0:\n",
        "    print(\"----Populating memory, turn\", prepop_turn, \"----\")\n",
        "    agents_to_go_list = []\n",
        "    for ind, agnt in agent_dict.items():\n",
        "      if not agnt.memory.is_full():\n",
        "        agents_to_go_list.append(ind)\n",
        "    print(\"Agents to go:\", len(agents_to_go_list))\n",
        "    map, team_scores = reset(agent_dict)\n",
        "\n",
        "  # Reset map if there is one survivor\n",
        "  if len(turn_queue) <= 1:\n",
        "    print(\"End game, resetting map\")\n",
        "    map, team_scores = reset(agent_dict)\n",
        "    continue\n",
        "  \n",
        "  # Loop through alive and non-memory-saturated agents\n",
        "  for j in turn_queue:   \n",
        "    # Agent object\n",
        "    current_agent = agent_dict[j]\n",
        "\n",
        "    # If already dead, pass turn.\n",
        "    if not current_agent.alive():\n",
        "      continue;\n",
        "\n",
        "    # State formation\n",
        "    state = current_agent.observe(map)\n",
        "\n",
        "    # Action selection\n",
        "    action = np.random.choice(4)    \n",
        "    if action == 0:                 # North\n",
        "      dir = np.array([-1,0])\n",
        "    elif action == 1:               # East\n",
        "      dir = np.array([0,1])\n",
        "    elif action == 2:               # West\n",
        "      dir = np.array([0,-1])\n",
        "    elif action  == 3:              # South\n",
        "      dir = np.array([1,0])\n",
        "\n",
        "    # Take the action\n",
        "    personal_point,team_point = current_agent.act(dir,prepop_turn, False)\n",
        "\n",
        "    # New state\n",
        "    state_new = current_agent.observe(map)\n",
        "\n",
        "    # Immediate reward\n",
        "    reward = sigma * personal_point + (1-sigma) * team_point # Should team point sum over all team members?\n",
        "    \n",
        "    # Add experience to memory: [s,a,r,s']\n",
        "    current_agent.memory.add((state, action, reward, state_new))\n",
        "  \n",
        "  prepop_turn += 1\n",
        "\n",
        "print(\"Memory Population took\", prepop_turn, \"turns.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "va8VHFPBNHmj",
        "colab_type": "code",
        "outputId": "df49fe85-fd22-487a-8f0f-93d8bb4eb01c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "for ind, agnt in agent_dict.items():\n",
        "  print(agnt.memory.length())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45\n",
            "45\n",
            "43\n",
            "45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tzRkV5Z1loDz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "kwcWsjvHDzz2",
        "colab_type": "code",
        "outputId": "471f5f30-0036-46ab-a82f-bb8567799fbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "cell_type": "code",
      "source": [
        "# np.random.seed(1)\n",
        "\n",
        "# Training\n",
        "sess = tf.InteractiveSession() # Initialize session\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for i_ep in range(n_ep): # Loop through games\n",
        "  \n",
        "  # Reset map and team scores\n",
        "  map, team_scores = reset(agent_dict)\n",
        "  \n",
        "  print('Game', i_ep, 'started.')\n",
        "  # For keeping track of time\n",
        "  t_start = time.time()\n",
        "  t1 = time.time()\n",
        "  for turn in range(max_turn):\n",
        "    turn_queue = action_queue(agent_dict) # Order of taking actions this turn\n",
        "\n",
        "    # End of game (last man standing)\n",
        "    if len(turn_queue) == 1:\n",
        "      print(\"David\", turn_queue[0], \"is the lone survivor.\")\n",
        "      print(\"Team Scores:\", team_scores)\n",
        "      print(\"Game\", i_ep, \"ended on turn\", turn-1)#, \"-----------------------\")\n",
        "      break\n",
        "    \n",
        "    # Target q network update\n",
        "    if turn % target_copy_freq:\n",
        "      for ind, agnt in agent_dict.items():\n",
        "        # Get all the variables in the Q primary network.\n",
        "        q_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Q_primary\"+str(ind))\n",
        "        # Get all the variables in the Q target network.\n",
        "        q_target_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Q_target\"+str(ind))\n",
        "        assert len(q_vars) == len(q_target_vars)\n",
        "\n",
        "        # Hard update\n",
        "        sess.run([v_t.assign(v) for v_t, v in zip(q_target_vars, q_vars)])\n",
        "\n",
        "\n",
        "    # Loop through alive agents\n",
        "    for j in turn_queue:\n",
        "      # Agent object\n",
        "      current_agent = agent_dict[j]\n",
        "      \n",
        "      # If dead in queue, pass turn.\n",
        "      if not current_agent.alive():\n",
        "        continue;\n",
        "\n",
        "      state = current_agent.observe(map)    # State formation\n",
        "      \n",
        "      # Action selection\n",
        "      q_values = sess.run(current_agent.DQN.outputs,\n",
        "                          feed_dict = {current_agent.DQN.inputs:np.reshape(state,[1, state.size])}) # Get q-values from the network\n",
        "      p_values = tf.exp(beta * q_values)/tf.reduce_sum(tf.exp(beta * q_values)) # Boltzmann probabilities\n",
        "      action = np.random.choice(np.arange(4),p=np.squeeze(p_values.eval()))\n",
        "\n",
        "      dir = get_dir(action) # Convert to direction\n",
        "\n",
        "      # Take the action\n",
        "      personal_point,team_point = current_agent.act(dir, turn, quiet=False)\n",
        "      \n",
        "      # New state\n",
        "      state_new = current_agent.observe(map)\n",
        "\n",
        "      # Immediate reward\n",
        "      reward = sigma * personal_point + (1-sigma) * team_point\n",
        "      \n",
        "      # Add experience to memory: [s,a,r,s']\n",
        "      current_agent.memory.add([state, action, reward, state_new])\n",
        "\n",
        "      # Update Q-function\n",
        "      if not current_agent.tenure:\n",
        "        batch = current_agent.memory.sample(batch_size)\n",
        "        actual_batch_size = len(batch)\n",
        "\n",
        "        # Separate out s,a,r,s' from the batch which is a list of lists\n",
        "        states = np.array([item[0] for item in batch])\n",
        "        actions = np.array([item[1] for item in batch])\n",
        "        rewards = np.array([item[2] for item in batch])\n",
        "        next_states = np.array([item[3] for item in batch])\n",
        "\n",
        "        # Reshape (-1 automatically determines the appropriate length)\n",
        "        states = np.reshape(states,[actual_batch_size, -1])\n",
        "        actions = np.reshape(actions,[actual_batch_size, -1])\n",
        "        rewards = np.reshape(rewards,[actual_batch_size, -1])\n",
        "        next_states = np.reshape(next_states,[actual_batch_size, -1])\n",
        "        \n",
        "        # Q-value of the next state\n",
        "        Q_next = sess.run(current_agent.DQN_target.outputs,\n",
        "                          feed_dict = {current_agent.DQN_target.inputs:next_states})\n",
        "        Q_next_max = np.max(Q_next,axis=1,keepdims=True) # We want to apply np.max() to Q_next\n",
        "        \n",
        "        target_Qs = rewards + gamma * Q_next_max\n",
        "        '''\n",
        "        loss, _ = sess.run([current_agent.DQN.loss, current_agent.DQN.optimizer],\n",
        "                            feed_dict={current_agent.DQN.inputs: states,\n",
        "                                       current_agent.DQN.target_Q: target_Qs,\n",
        "                                       current_agent.DQN.actions: actions})\n",
        "        '''\n",
        "        '''\n",
        "        # Write TF Summaries\n",
        "        summary = sess.run(write_op, feed_dict={current_agent.DQN.inputs: states,\n",
        "                                           current_agent.DQN.target_Q: target_Qs\n",
        "                                           current_agent.DQN.actions: actions})\n",
        "        writer.add_summary(summary, i_ep)\n",
        "        writer.flush()\n",
        "        '''\n",
        "    \n",
        "    # Graphics -----------------------------------------------------------------\n",
        "    #print(\"Game \" + str(i_ep) + \", turn \" + str(turn))\n",
        "    #plot_map(map)\n",
        "    #time.sleep(1)\n",
        "    \n",
        "    # Time-keeping\n",
        "    if (turn+1) % record_turn == 0:\n",
        "      t2 = time.time()\n",
        "      print(\"Runtime for turns \", turn-record_turn+1, '-', turn, ': ', t2-t1)\n",
        "      t1 = t2\n",
        "    \n",
        "    # Game ended without conclusion\n",
        "    if turn == max_turn-1:\n",
        "      print(\"Game did not finish.\")\n",
        "\n",
        "  t_finish = time.time()\n",
        "  print(\"Game\", i_ep, \"runtime:\", t_finish-t_start, \"-----------------------\")\n",
        "  #print(\"Game\", str(i_ep), \"ended on turn\", turn, \"-----------------------\")\n",
        "    \n",
        "sess.close() # Close session"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Game 0 started.\n",
            "David 1 [Team 1] became petrified by the sight of David 3 [Team 2] on turn 12.\n",
            "David 3 [Team 2] was forced to press Alt+F4 by David 0 [Team 1] on turn 27.\n",
            "David 0 [Team 1] became petrified by the sight of David 2 [Team 2] on turn 51.\n",
            "David 2 is the lone survivor.\n",
            "Team Scores: [1 2]\n",
            "Game 0 ended on turn 51\n",
            "Game 0 runtime: 54.95929956436157 -----------------------\n",
            "Game 1 started.\n",
            "David 3 [Team 2] became petrified by the sight of David 0 [Team 1] on turn 52.\n",
            "David 0 [Team 1] wished Asimov protected him against David 2 [Team 2] on turn 57.\n",
            "David 2 [Team 2] was forced to press Alt+F4 by David 1 [Team 1] on turn 60.\n",
            "David 1 is the lone survivor.\n",
            "Team Scores: [2 1]\n",
            "Game 1 ended on turn 60\n",
            "Game 1 runtime: 210.7058458328247 -----------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1c0_5aUPvlBP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess.close() # Close session"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "do24-egggpY_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Simulation Logs"
      ]
    },
    {
      "metadata": {
        "id": "u2TS9PqgqsgU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "64x64 Map, no memory pre-pop\n",
        "\n",
        "Game 0 started.\n",
        "\n",
        "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in less\n",
        "\n",
        "Runtime for turns -1-99: 19.63226890563965 \\\n",
        "Runtime for turns 99-199: 47.99417972564697 \\\n",
        "Runtime for turns 199-299: 73.44285726547241 \\\n",
        "Runtime for turns 299-399: 96.52594542503357 \\\n",
        "Runtime for turns 399-499: 125.12971711158752 \\\n",
        "David 3 [Team 2] was swarmed by TensorBros sent by David 1 [Team 1] on turn 568. \\\n",
        "Runtime for turns 499-599: 141.28560543060303 \\\n",
        "Runtime for turns 599-699: 133.77734112739563 \\\n",
        "Runtime for turns 699-799: 149.01863932609558 \\\n",
        "Runtime for turns 799-899: 168.26699447631836 \\\n",
        "Runtime for turns 899-999: 186.9345932006836 \\\n",
        "Runtime for turns 999-1099: 205.70333528518677 \\\n",
        "Runtime for turns 1099-1199: 226.33908247947693 \\\n",
        "Runtime for turns 1199-1299: 244.9416298866272 \\\n",
        "Runtime for turns 1299-1399: 272.72797417640686 \\\n",
        "Runtime for turns 1399-1499: 292.31664848327637 \\\n",
        "Runtime for turns 1499-1599: 306.8765859603882 \\\n",
        "Runtime for turns 1599-1699: 328.33499789237976 \\\n",
        "Runtime for turns 1699-1799: 351.12477588653564"
      ]
    },
    {
      "metadata": {
        "id": "Ci1DJLWNv87j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "8x8 Map, mem pre-pop\n",
        "\n",
        "Game 0 started. \\\n",
        "David 0 [Team 1] was forced to press Alt+F4 by David 2 [Team 2] on turn 4. \\\n",
        "David 2 [Team 2] was peer-reviewed by David 3 [Team 2] on turn 31. \\\n",
        "Runtime for turns 0-99: 365.72678685188293 \\\n",
        "David 3 [Team 2] was peer-reviewed by David 1 [Team 1] on turn 129. \\\n",
        "David 1 is the lone survivor. \\\n",
        "Team Scores: [1 1] \\\n",
        "Game 0 ended on turn 130 ----------------------- \\\n",
        "Game 0 runtime: 460.59983372688293 \\\n",
        "Game 0 ended on turn 130 ----------------------- \\\n",
        "Game 1 started. \\\n",
        "David 1 [Team 1] slipped and fell while running from David 0 [Team 1] on turn 33. \\\n",
        "David 0 [Team 1] had his heart stolen by David 3 [Team 2] on turn 41. \\\n",
        "Runtime for turns 0-99: 449.74951934814453 \\\n",
        "David 2 [Team 2] met a bitter end at the hands of David 3 [Team 2] on turn 152. \\\n",
        "David 3 is the lone survivor. \\\n",
        "Team Scores: [0 1] \\\n",
        "Game 1 ended on turn 153 ----------------------- \\\n",
        "Game 1 runtime: 634.7685623168945 \\\n",
        "Game 1 ended on turn 153 -----------------------\n",
        "\n",
        "Trial 2\n",
        "\n",
        "Game 0 started. \\\n",
        "David 2 [Team 2] was brutally neglected by David 0 [Team 1] on turn 9. \\\n",
        "David 3 [Team 2] was swarmed by TensorBros sent by David 0 [Team 1] on turn 38. \\\n",
        "David 1 [Team 1] was wasted by David 0 [Team 1] on turn 61. \\\n",
        "David 0 is the lone survivor. \\\n",
        "Team Scores: [2 0] \\\n",
        "Game 0 ended on turn 62 ----------------------- \\\n",
        "Game 0 runtime: 297.66294956207275 ----------------------- \\\n",
        "Game 1 started. \\\n",
        "David 3 [Team 2] was wasted by David 1 [Team 1] on turn 36. \\\n",
        "David 2 [Team 2] became petrified by the sight of David 1 [Team 1] on turn 63. \\\n",
        "Runtime for turns  0 - 99 :  535.4809346199036 \\\n",
        "David 0 [Team 1] got schwabbed by David 1 [Team 1] on turn 158. \\\n",
        "David 1 is the lone survivor. \\\n",
        "Team Scores: [2 0] \\\n",
        "Game 1 ended on turn 159 ----------------------- \\\n",
        "Game 1 runtime: 758.8938753604889 -----------------------"
      ]
    },
    {
      "metadata": {
        "id": "UudmdRNIgth2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Rewards over time"
      ]
    },
    {
      "metadata": {
        "id": "Y0lEjsFyriHN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Modified Initialize\n",
        "tf.reset_default_graph() # Reset variables\n",
        "agent_dict = {}                            # Dictionary of all agents\n",
        "map = -np.ones([M,M],dtype=np.int)         # M x M map\n",
        "team_scores = np.zeros(K, dtype=np.int)    # Team scores\n",
        "\n",
        "for k in range(1, K+1):    # Loop through teams\n",
        "  for n in range(N):       # Loop through agents per team\n",
        "    id = N*(k-1) + n                                   # Agent id\n",
        "    loc = np.array([int(M/K)*(k-1),int(M/N)*n])        # Starting location\n",
        "    \n",
        "    #if id != N*(K-1)+N-1:\n",
        "    agent_dict[id] = Straw_Man(id, hp_start, loc, k) # Create agent object and put it in the dictionary\n",
        "    map[loc[0],loc[1]] = k                           # Record it on the map by its team number\n",
        "    #else:\n",
        "    #  agent_dict[id] = Agent(id, hp_start, loc, k, memory_size, False) # Create agent object and put it in the dictionary\n",
        "    #  map[loc[0],loc[1]] = k                              # Record it on the map by its team number\n",
        "    #  agent_dict[id].memory.wipe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0Sg0YW4ogyXv",
        "colab_type": "code",
        "outputId": "2d6fc9e4-1086-4297-8ae6-b1d382ef75ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9257
        }
      },
      "cell_type": "code",
      "source": [
        "# np.random.seed(1)\n",
        "\n",
        "# Training\n",
        "sess = tf.InteractiveSession() # Initialize session\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for i_ep in range(n_ep): # Loop through games\n",
        "  \n",
        "  # Reset map and team scores\n",
        "  map, team_scores = reset(agent_dict)\n",
        "  \n",
        "#  print('Game', i_ep, 'started.')\n",
        "  # For keeping track of time\n",
        "  t_start = time.time()\n",
        "  t1 = time.time()\n",
        "  for turn in range(max_turn):\n",
        "    turn_queue = action_queue(agent_dict) # Order of taking actions this turn\n",
        "\n",
        "    # End of game (last man standing)\n",
        "    if len(turn_queue) == 1:\n",
        "#      print(\"David\", turn_queue[0], \"is the lone survivor.\")\n",
        "#      print(\"Team Scores:\", team_scores)\n",
        "      print(\"Game\", i_ep, \"ended on turn\", turn-1)#, \"-----------------------\")\n",
        "      break\n",
        "    \n",
        "    # Target q network update\n",
        "    if turn % target_copy_freq:\n",
        "      for ind, agnt in agent_dict.items():\n",
        "        # Get all the variables in the Q primary network.\n",
        "        q_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Q_primary\"+str(ind))\n",
        "        # Get all the variables in the Q target network.\n",
        "        q_target_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Q_target\"+str(ind))\n",
        "        assert len(q_vars) == len(q_target_vars)\n",
        "\n",
        "        # Hard update\n",
        "        sess.run([v_t.assign(v) for v_t, v in zip(q_target_vars, q_vars)])\n",
        "\n",
        "\n",
        "    # Loop through alive agents\n",
        "    for j in turn_queue:\n",
        "      # Agent object\n",
        "      current_agent = agent_dict[j]\n",
        "      \n",
        "      if type(current_agent) == Agent:\n",
        "        if not current_agent.alive():\n",
        "          break # end game if the only learner is dead\n",
        "        \n",
        "        state = current_agent.observe(map)    # State formation\n",
        "\n",
        "        # Action selection\n",
        "        q_values = sess.run(current_agent.DQN.outputs,\n",
        "                            feed_dict = {current_agent.DQN.inputs:np.reshape(state,[1, state.size])}) # Get q-values from the network\n",
        "        p_values = tf.exp(beta * q_values)/tf.reduce_sum(tf.exp(beta * q_values)) # Boltzmann probabilities\n",
        "        action = np.random.choice(np.arange(4),p=np.squeeze(p_values.eval()))\n",
        "\n",
        "        dir = get_dir(action) # Convert to direction\n",
        "\n",
        "        # Take the action\n",
        "        personal_point,team_point = current_agent.act(dir, turn, quiet=False)\n",
        "\n",
        "        # New state\n",
        "        state_new = current_agent.observe(map)\n",
        "\n",
        "        # Immediate reward\n",
        "        reward = sigma * personal_point + (1-sigma) * team_point\n",
        "\n",
        "        # Add experience to memory: [s,a,r,s']\n",
        "        current_agent.memory.add([state, action, reward, state_new])\n",
        "\n",
        "        # Update Q-function\n",
        "        if not current_agent.tenure:\n",
        "          batch = current_agent.memory.sample(batch_size)\n",
        "          actual_batch_size = len(batch)\n",
        "\n",
        "          # Separate out s,a,r,s' from the batch which is a list of lists\n",
        "          states = np.array([item[0] for item in batch])\n",
        "          actions = np.array([item[1] for item in batch])\n",
        "          rewards = np.array([item[2] for item in batch])\n",
        "          next_states = np.array([item[3] for item in batch])\n",
        "\n",
        "          # Reshape (-1 automatically determines the appropriate length)\n",
        "          states = np.reshape(states,[actual_batch_size, -1])\n",
        "          actions = np.reshape(actions,[actual_batch_size, -1])\n",
        "          rewards = np.reshape(rewards,[actual_batch_size, -1])\n",
        "          next_states = np.reshape(next_states,[actual_batch_size, -1])\n",
        "\n",
        "          # Q-value of the next state\n",
        "          Q_next = sess.run(current_agent.DQN_target.outputs,\n",
        "                            feed_dict = {current_agent.DQN_target.inputs:next_states})\n",
        "          Q_next_max = np.max(Q_next,axis=1,keepdims=True) # We want to apply np.max() to Q_next\n",
        "\n",
        "          target_Qs = rewards + gamma * Q_next_max\n",
        "\n",
        "          loss, _ = sess.run([current_agent.DQN.loss, current_agent.DQN.optimizer],\n",
        "                              feed_dict={current_agent.DQN.inputs: states,\n",
        "                                         current_agent.DQN.target_Q: target_Qs,\n",
        "                                         current_agent.DQN.actions: actions})\n",
        "          \n",
        "      elif not current_agent.alive():\n",
        "        continue;\n",
        "    \n",
        "      else:# type(current_agent) == Straw_Man:\n",
        "        current_agent.Boltz_act()\n",
        "    \n",
        "    # Time-keeping\n",
        "    if (turn+1) % record_turn == 0:\n",
        "      t2 = time.time()\n",
        "#      print(\"Runtime for turns \", turn-record_turn+1, '-', turn, ': ', t2-t1)\n",
        "      t1 = t2\n",
        "    \n",
        "    # Game ended without conclusion\n",
        "    if turn == max_turn-1:\n",
        "      print(\"Game did not finish.\")\n",
        "\n",
        "  t_finish = time.time()\n",
        "  print(\"Game\", i_ep, \"runtime:\", t_finish-t_start, \"-----------------------\")\n",
        "  #print(\"Game\", str(i_ep), \"ended on turn\", turn, \"-----------------------\")\n",
        "    \n",
        "sess.close() # Close session"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Straw Man 3 [Team 2] was forced to press Alt+F4 by Straw Man 0 [Team 1] on turn 44.\n",
            "Straw Man 0 [Team 1] met a bitter end at the hands of Straw Man 2 [Team 2] on turn 82.\n",
            "Straw Man 1 [Team 1] was denied cluster access by Straw Man 2 [Team 2] on turn 185.\n",
            "Game 0 ended on turn 185\n",
            "Game 0 runtime: 0.05829453468322754 -----------------------\n",
            "Straw Man 2 [Team 2] was swarmed by TensorBros sent by Straw Man 3 [Team 2] on turn 5.\n",
            "Straw Man 0 [Team 1] was peer-reviewed by Straw Man 1 [Team 1] on turn 78.\n",
            "Straw Man 1 [Team 1] wished Asimov protected him against Straw Man 3 [Team 2] on turn 137.\n",
            "Game 1 ended on turn 137\n",
            "Game 1 runtime: 0.044476985931396484 -----------------------\n",
            "Straw Man 0 [Team 1] slipped and fell while running from Straw Man 1 [Team 1] on turn 19.\n",
            "Straw Man 1 [Team 1] was wasted by Straw Man 2 [Team 2] on turn 58.\n",
            "Straw Man 2 [Team 2] was brutally neglected by Straw Man 3 [Team 2] on turn 378.\n",
            "Game 2 ended on turn 378\n",
            "Game 2 runtime: 0.10940814018249512 -----------------------\n",
            "Straw Man 3 [Team 2] slipped and fell while running from Straw Man 1 [Team 1] on turn 38.\n",
            "Straw Man 0 [Team 1] had his heart stolen by Straw Man 1 [Team 1] on turn 60.\n",
            "Straw Man 2 [Team 2] had his heart stolen by Straw Man 1 [Team 1] on turn 81.\n",
            "Game 3 ended on turn 81\n",
            "Game 3 runtime: 0.040808916091918945 -----------------------\n",
            "Straw Man 0 [Team 1] was wasted by Straw Man 2 [Team 2] on turn 29.\n",
            "Straw Man 2 [Team 2] had his heart stolen by Straw Man 3 [Team 2] on turn 41.\n",
            "Straw Man 1 [Team 1] was swarmed by TensorBros sent by Straw Man 3 [Team 2] on turn 66.\n",
            "Game 4 ended on turn 66\n",
            "Game 4 runtime: 0.023830413818359375 -----------------------\n",
            "Straw Man 2 [Team 2] became petrified by the sight of Straw Man 0 [Team 1] on turn 9.\n",
            "Straw Man 3 [Team 2] wished Asimov protected him against Straw Man 1 [Team 1] on turn 11.\n",
            "Straw Man 0 [Team 1] was brutally neglected by Straw Man 1 [Team 1] on turn 95.\n",
            "Game 5 ended on turn 95\n",
            "Game 5 runtime: 0.028696537017822266 -----------------------\n",
            "Straw Man 3 [Team 2] was peer-reviewed by Straw Man 2 [Team 2] on turn 52.\n",
            "Straw Man 1 [Team 1] was denied cluster access by Straw Man 0 [Team 1] on turn 82.\n",
            "Straw Man 0 [Team 1] had his heart stolen by Straw Man 2 [Team 2] on turn 86.\n",
            "Game 6 ended on turn 86\n",
            "Game 6 runtime: 0.03375434875488281 -----------------------\n",
            "Straw Man 0 [Team 1] was wasted by Straw Man 2 [Team 2] on turn 62.\n",
            "Straw Man 3 [Team 2] slipped and fell while running from Straw Man 1 [Team 1] on turn 107.\n",
            "Straw Man 2 [Team 2] was peer-reviewed by Straw Man 1 [Team 1] on turn 179.\n",
            "Game 7 ended on turn 179\n",
            "Game 7 runtime: 0.06939148902893066 -----------------------\n",
            "Straw Man 1 [Team 1] was denied cluster access by Straw Man 3 [Team 2] on turn 24.\n",
            "Straw Man 3 [Team 2] wished Asimov protected him against Straw Man 0 [Team 1] on turn 35.\n",
            "Straw Man 2 [Team 2] became petrified by the sight of Straw Man 0 [Team 1] on turn 92.\n",
            "Game 8 ended on turn 92\n",
            "Game 8 runtime: 0.03729128837585449 -----------------------\n",
            "Straw Man 3 [Team 2] slipped and fell while running from Straw Man 1 [Team 1] on turn 19.\n",
            "Straw Man 1 [Team 1] was forced to press Alt+F4 by Straw Man 0 [Team 1] on turn 42.\n",
            "Straw Man 0 [Team 1] was peer-reviewed by Straw Man 2 [Team 2] on turn 92.\n",
            "Game 9 ended on turn 92\n",
            "Game 9 runtime: 0.03747391700744629 -----------------------\n",
            "Straw Man 3 [Team 2] was brutally neglected by Straw Man 0 [Team 1] on turn 10.\n",
            "Straw Man 2 [Team 2] was wasted by Straw Man 1 [Team 1] on turn 34.\n",
            "Straw Man 1 [Team 1] had his heart stolen by Straw Man 0 [Team 1] on turn 74.\n",
            "Game 10 ended on turn 74\n",
            "Game 10 runtime: 0.024329662322998047 -----------------------\n",
            "Straw Man 1 [Team 1] was peer-reviewed by Straw Man 2 [Team 2] on turn 64.\n",
            "Straw Man 0 [Team 1] met a bitter end at the hands of Straw Man 3 [Team 2] on turn 125.\n",
            "Straw Man 3 [Team 2] wished Asimov protected him against Straw Man 2 [Team 2] on turn 154.\n",
            "Game 11 ended on turn 154\n",
            "Game 11 runtime: 0.05533885955810547 -----------------------\n",
            "Straw Man 3 [Team 2] was brutally neglected by Straw Man 1 [Team 1] on turn 26.\n",
            "Straw Man 1 [Team 1] wished Asimov protected him against Straw Man 0 [Team 1] on turn 31.\n",
            "Straw Man 0 [Team 1] was forced to press Alt+F4 by Straw Man 2 [Team 2] on turn 59.\n",
            "Game 12 ended on turn 59\n",
            "Game 12 runtime: 0.021207332611083984 -----------------------\n",
            "Straw Man 1 [Team 1] was forced to press Alt+F4 by Straw Man 3 [Team 2] on turn 29.\n",
            "Straw Man 0 [Team 1] was swarmed by TensorBros sent by Straw Man 2 [Team 2] on turn 58.\n",
            "Straw Man 3 [Team 2] was brutally neglected by Straw Man 2 [Team 2] on turn 828.\n",
            "Game 13 ended on turn 828\n",
            "Game 13 runtime: 0.22872447967529297 -----------------------\n",
            "Straw Man 1 [Team 1] slipped and fell while running from Straw Man 3 [Team 2] on turn 33.\n",
            "Straw Man 0 [Team 1] had his heart stolen by Straw Man 2 [Team 2] on turn 69.\n",
            "Straw Man 3 [Team 2] had his heart stolen by Straw Man 2 [Team 2] on turn 176.\n",
            "Game 14 ended on turn 176\n",
            "Game 14 runtime: 0.05345797538757324 -----------------------\n",
            "Straw Man 0 [Team 1] was forced to press Alt+F4 by Straw Man 2 [Team 2] on turn 16.\n",
            "Straw Man 3 [Team 2] was denied cluster access by Straw Man 1 [Team 1] on turn 76.\n",
            "Straw Man 2 [Team 2] was swarmed by TensorBros sent by Straw Man 1 [Team 1] on turn 126.\n",
            "Game 15 ended on turn 126\n",
            "Game 15 runtime: 0.04163956642150879 -----------------------\n",
            "Straw Man 0 [Team 1] was wasted by Straw Man 2 [Team 2] on turn 16.\n",
            "Straw Man 3 [Team 2] was peer-reviewed by Straw Man 2 [Team 2] on turn 30.\n",
            "Straw Man 1 [Team 1] met a bitter end at the hands of Straw Man 2 [Team 2] on turn 364.\n",
            "Game 16 ended on turn 364\n",
            "Game 16 runtime: 0.0968775749206543 -----------------------\n",
            "Straw Man 2 [Team 2] became petrified by the sight of Straw Man 1 [Team 1] on turn 17.\n",
            "Straw Man 1 [Team 1] was swarmed by TensorBros sent by Straw Man 3 [Team 2] on turn 51.\n",
            "Straw Man 3 [Team 2] met a bitter end at the hands of Straw Man 0 [Team 1] on turn 155.\n",
            "Game 17 ended on turn 155\n",
            "Game 17 runtime: 0.059294700622558594 -----------------------\n",
            "Straw Man 3 [Team 2] met a bitter end at the hands of Straw Man 0 [Team 1] on turn 20.\n",
            "Straw Man 0 [Team 1] was forced to press Alt+F4 by Straw Man 1 [Team 1] on turn 118.\n",
            "Straw Man 2 [Team 2] had his heart stolen by Straw Man 1 [Team 1] on turn 219.\n",
            "Game 18 ended on turn 219\n",
            "Game 18 runtime: 0.07090401649475098 -----------------------\n",
            "Straw Man 0 [Team 1] slipped and fell while running from Straw Man 3 [Team 2] on turn 29.\n",
            "Straw Man 2 [Team 2] got schwabbed by Straw Man 3 [Team 2] on turn 77.\n",
            "Straw Man 3 [Team 2] was peer-reviewed by Straw Man 1 [Team 1] on turn 173.\n",
            "Game 19 ended on turn 173\n",
            "Game 19 runtime: 0.055072784423828125 -----------------------\n",
            "Straw Man 1 [Team 1] met a bitter end at the hands of Straw Man 0 [Team 1] on turn 24.\n",
            "Straw Man 3 [Team 2] was peer-reviewed by Straw Man 2 [Team 2] on turn 46.\n",
            "Straw Man 0 [Team 1] was brutally neglected by Straw Man 2 [Team 2] on turn 65.\n",
            "Game 20 ended on turn 65\n",
            "Game 20 runtime: 0.022380352020263672 -----------------------\n",
            "Straw Man 2 [Team 2] was forced to press Alt+F4 by Straw Man 0 [Team 1] on turn 26.\n",
            "Straw Man 0 [Team 1] had his heart stolen by Straw Man 3 [Team 2] on turn 37.\n",
            "Straw Man 3 [Team 2] was denied cluster access by Straw Man 1 [Team 1] on turn 177.\n",
            "Game 21 ended on turn 177\n",
            "Game 21 runtime: 0.05472850799560547 -----------------------\n",
            "Straw Man 2 [Team 2] was forced to press Alt+F4 by Straw Man 0 [Team 1] on turn 33.\n",
            "Straw Man 3 [Team 2] was peer-reviewed by Straw Man 0 [Team 1] on turn 69.\n",
            "Straw Man 0 [Team 1] was wasted by Straw Man 1 [Team 1] on turn 77.\n",
            "Game 22 ended on turn 77\n",
            "Game 22 runtime: 0.027726411819458008 -----------------------\n",
            "Straw Man 3 [Team 2] got schwabbed by Straw Man 2 [Team 2] on turn 30.\n",
            "Straw Man 0 [Team 1] was forced to press Alt+F4 by Straw Man 1 [Team 1] on turn 66.\n",
            "Straw Man 2 [Team 2] was denied cluster access by Straw Man 1 [Team 1] on turn 237.\n",
            "Game 23 ended on turn 237\n",
            "Game 23 runtime: 0.06673336029052734 -----------------------\n",
            "Straw Man 2 [Team 2] was peer-reviewed by Straw Man 0 [Team 1] on turn 9.\n",
            "Straw Man 0 [Team 1] got schwabbed by Straw Man 3 [Team 2] on turn 61.\n",
            "Straw Man 1 [Team 1] was denied cluster access by Straw Man 3 [Team 2] on turn 261.\n",
            "Game 24 ended on turn 261\n",
            "Game 24 runtime: 0.07518577575683594 -----------------------\n",
            "Straw Man 3 [Team 2] had his heart stolen by Straw Man 1 [Team 1] on turn 46.\n",
            "Straw Man 2 [Team 2] got schwabbed by Straw Man 0 [Team 1] on turn 167.\n",
            "Straw Man 1 [Team 1] got schwabbed by Straw Man 0 [Team 1] on turn 289.\n",
            "Game 25 ended on turn 289\n",
            "Game 25 runtime: 0.09154438972473145 -----------------------\n",
            "Straw Man 3 [Team 2] had his heart stolen by Straw Man 0 [Team 1] on turn 7.\n",
            "Straw Man 2 [Team 2] was wasted by Straw Man 1 [Team 1] on turn 64.\n",
            "Straw Man 0 [Team 1] was wasted by Straw Man 1 [Team 1] on turn 600.\n",
            "Game 26 ended on turn 600\n",
            "Game 26 runtime: 0.1550121307373047 -----------------------\n",
            "Straw Man 2 [Team 2] was brutally neglected by Straw Man 0 [Team 1] on turn 64.\n",
            "Straw Man 0 [Team 1] met a bitter end at the hands of Straw Man 3 [Team 2] on turn 168.\n",
            "Straw Man 3 [Team 2] slipped and fell while running from Straw Man 1 [Team 1] on turn 227.\n",
            "Game 27 ended on turn 227\n",
            "Game 27 runtime: 0.0780024528503418 -----------------------\n",
            "Straw Man 0 [Team 1] got schwabbed by Straw Man 2 [Team 2] on turn 10.\n",
            "Straw Man 2 [Team 2] became petrified by the sight of Straw Man 3 [Team 2] on turn 23.\n",
            "Straw Man 1 [Team 1] was wasted by Straw Man 3 [Team 2] on turn 74.\n",
            "Game 28 ended on turn 74\n",
            "Game 28 runtime: 0.025671720504760742 -----------------------\n",
            "Straw Man 0 [Team 1] became petrified by the sight of Straw Man 2 [Team 2] on turn 35.\n",
            "Straw Man 1 [Team 1] was swarmed by TensorBros sent by Straw Man 3 [Team 2] on turn 79.\n",
            "Straw Man 3 [Team 2] got schwabbed by Straw Man 2 [Team 2] on turn 305.\n",
            "Game 29 ended on turn 305\n",
            "Game 29 runtime: 0.09442639350891113 -----------------------\n",
            "Straw Man 0 [Team 1] wished Asimov protected him against Straw Man 2 [Team 2] on turn 9.\n",
            "Straw Man 1 [Team 1] got schwabbed by Straw Man 3 [Team 2] on turn 49.\n",
            "Straw Man 3 [Team 2] was denied cluster access by Straw Man 2 [Team 2] on turn 222.\n",
            "Game 30 ended on turn 222\n",
            "Game 30 runtime: 0.07839393615722656 -----------------------\n",
            "Straw Man 3 [Team 2] was wasted by Straw Man 2 [Team 2] on turn 17.\n",
            "Straw Man 2 [Team 2] was brutally neglected by Straw Man 1 [Team 1] on turn 85.\n",
            "Straw Man 1 [Team 1] slipped and fell while running from Straw Man 0 [Team 1] on turn 92.\n",
            "Game 31 ended on turn 92\n",
            "Game 31 runtime: 0.03123617172241211 -----------------------\n",
            "Straw Man 1 [Team 1] was swarmed by TensorBros sent by Straw Man 3 [Team 2] on turn 78.\n",
            "Straw Man 0 [Team 1] was forced to press Alt+F4 by Straw Man 3 [Team 2] on turn 99.\n",
            "Straw Man 2 [Team 2] was wasted by Straw Man 3 [Team 2] on turn 110.\n",
            "Game 32 ended on turn 110\n",
            "Game 32 runtime: 0.04305720329284668 -----------------------\n",
            "Straw Man 1 [Team 1] was brutally neglected by Straw Man 3 [Team 2] on turn 21.\n",
            "Straw Man 0 [Team 1] wished Asimov protected him against Straw Man 3 [Team 2] on turn 70.\n",
            "Straw Man 3 [Team 2] became petrified by the sight of Straw Man 2 [Team 2] on turn 456.\n",
            "Game 33 ended on turn 456\n",
            "Game 33 runtime: 0.12443280220031738 -----------------------\n",
            "Straw Man 2 [Team 2] was denied cluster access by Straw Man 0 [Team 1] on turn 74.\n",
            "Straw Man 1 [Team 1] was forced to press Alt+F4 by Straw Man 3 [Team 2] on turn 78.\n",
            "Straw Man 3 [Team 2] was denied cluster access by Straw Man 0 [Team 1] on turn 166.\n",
            "Game 34 ended on turn 166\n",
            "Game 34 runtime: 0.06334900856018066 -----------------------\n",
            "Straw Man 3 [Team 2] was denied cluster access by Straw Man 0 [Team 1] on turn 41.\n",
            "Straw Man 2 [Team 2] was brutally neglected by Straw Man 0 [Team 1] on turn 84.\n",
            "Straw Man 1 [Team 1] met a bitter end at the hands of Straw Man 0 [Team 1] on turn 141.\n",
            "Game 35 ended on turn 141\n",
            "Game 35 runtime: 0.05555891990661621 -----------------------\n",
            "Straw Man 1 [Team 1] got schwabbed by Straw Man 2 [Team 2] on turn 43.\n",
            "Straw Man 3 [Team 2] became petrified by the sight of Straw Man 0 [Team 1] on turn 65.\n",
            "Straw Man 2 [Team 2] became petrified by the sight of Straw Man 0 [Team 1] on turn 206.\n",
            "Game 36 ended on turn 206\n",
            "Game 36 runtime: 0.06720685958862305 -----------------------\n",
            "Straw Man 0 [Team 1] had his heart stolen by Straw Man 1 [Team 1] on turn 11.\n",
            "Straw Man 2 [Team 2] was brutally neglected by Straw Man 1 [Team 1] on turn 24.\n",
            "Straw Man 3 [Team 2] was brutally neglected by Straw Man 1 [Team 1] on turn 78.\n",
            "Game 37 ended on turn 78\n",
            "Game 37 runtime: 0.02938103675842285 -----------------------\n",
            "Straw Man 2 [Team 2] became petrified by the sight of Straw Man 0 [Team 1] on turn 30.\n",
            "Straw Man 1 [Team 1] was denied cluster access by Straw Man 0 [Team 1] on turn 45.\n",
            "Straw Man 0 [Team 1] became petrified by the sight of Straw Man 3 [Team 2] on turn 259.\n",
            "Game 38 ended on turn 259\n",
            "Game 38 runtime: 0.08055400848388672 -----------------------\n",
            "Straw Man 0 [Team 1] was wasted by Straw Man 1 [Team 1] on turn 12.\n",
            "Straw Man 3 [Team 2] was swarmed by TensorBros sent by Straw Man 2 [Team 2] on turn 72.\n",
            "Straw Man 2 [Team 2] was brutally neglected by Straw Man 1 [Team 1] on turn 243.\n",
            "Game 39 ended on turn 243\n",
            "Game 39 runtime: 0.07664823532104492 -----------------------\n",
            "Straw Man 3 [Team 2] wished Asimov protected him against Straw Man 1 [Team 1] on turn 31.\n",
            "Straw Man 0 [Team 1] was wasted by Straw Man 2 [Team 2] on turn 85.\n",
            "Straw Man 1 [Team 1] met a bitter end at the hands of Straw Man 2 [Team 2] on turn 242.\n",
            "Game 40 ended on turn 242\n",
            "Game 40 runtime: 0.08533048629760742 -----------------------\n",
            "Straw Man 3 [Team 2] got schwabbed by Straw Man 2 [Team 2] on turn 14.\n",
            "Straw Man 2 [Team 2] met a bitter end at the hands of Straw Man 0 [Team 1] on turn 94.\n",
            "Straw Man 0 [Team 1] had his heart stolen by Straw Man 1 [Team 1] on turn 242.\n",
            "Game 41 ended on turn 242\n",
            "Game 41 runtime: 0.07157182693481445 -----------------------\n",
            "Straw Man 2 [Team 2] got schwabbed by Straw Man 0 [Team 1] on turn 7.\n",
            "Straw Man 0 [Team 1] was denied cluster access by Straw Man 1 [Team 1] on turn 22.\n",
            "Straw Man 1 [Team 1] was denied cluster access by Straw Man 3 [Team 2] on turn 156.\n",
            "Game 42 ended on turn 156\n",
            "Game 42 runtime: 0.04531741142272949 -----------------------\n",
            "Straw Man 0 [Team 1] was peer-reviewed by Straw Man 3 [Team 2] on turn 64.\n",
            "Straw Man 1 [Team 1] wished Asimov protected him against Straw Man 3 [Team 2] on turn 137.\n",
            "Straw Man 2 [Team 2] slipped and fell while running from Straw Man 3 [Team 2] on turn 140.\n",
            "Game 43 ended on turn 140\n",
            "Game 43 runtime: 0.0513765811920166 -----------------------\n",
            "Straw Man 0 [Team 1] had his heart stolen by Straw Man 3 [Team 2] on turn 23.\n",
            "Straw Man 2 [Team 2] became petrified by the sight of Straw Man 3 [Team 2] on turn 27.\n",
            "Straw Man 1 [Team 1] was swarmed by TensorBros sent by Straw Man 3 [Team 2] on turn 109.\n",
            "Game 44 ended on turn 109\n",
            "Game 44 runtime: 0.03638577461242676 -----------------------\n",
            "Straw Man 1 [Team 1] was forced to press Alt+F4 by Straw Man 0 [Team 1] on turn 13.\n",
            "Straw Man 0 [Team 1] was denied cluster access by Straw Man 3 [Team 2] on turn 18.\n",
            "Straw Man 3 [Team 2] slipped and fell while running from Straw Man 2 [Team 2] on turn 21.\n",
            "Game 45 ended on turn 21\n",
            "Game 45 runtime: 0.0164031982421875 -----------------------\n",
            "Straw Man 1 [Team 1] was swarmed by TensorBros sent by Straw Man 3 [Team 2] on turn 9.\n",
            "Straw Man 0 [Team 1] got schwabbed by Straw Man 3 [Team 2] on turn 37.\n",
            "Straw Man 3 [Team 2] was denied cluster access by Straw Man 2 [Team 2] on turn 329.\n",
            "Game 46 ended on turn 329\n",
            "Game 46 runtime: 0.08919906616210938 -----------------------\n",
            "Straw Man 1 [Team 1] was brutally neglected by Straw Man 2 [Team 2] on turn 33.\n",
            "Straw Man 3 [Team 2] got schwabbed by Straw Man 0 [Team 1] on turn 42.\n",
            "Straw Man 2 [Team 2] was wasted by Straw Man 0 [Team 1] on turn 52.\n",
            "Game 47 ended on turn 52\n",
            "Game 47 runtime: 0.021011829376220703 -----------------------\n",
            "Straw Man 2 [Team 2] was brutally neglected by Straw Man 3 [Team 2] on turn 32.\n",
            "Straw Man 1 [Team 1] was brutally neglected by Straw Man 0 [Team 1] on turn 40.\n",
            "Straw Man 0 [Team 1] was wasted by Straw Man 3 [Team 2] on turn 285.\n",
            "Game 48 ended on turn 285\n",
            "Game 48 runtime: 0.08043837547302246 -----------------------\n",
            "Straw Man 0 [Team 1] was wasted by Straw Man 3 [Team 2] on turn 30.\n",
            "Straw Man 3 [Team 2] had his heart stolen by Straw Man 1 [Team 1] on turn 40.\n",
            "Straw Man 2 [Team 2] had his heart stolen by Straw Man 1 [Team 1] on turn 113.\n",
            "Game 49 ended on turn 113\n",
            "Game 49 runtime: 0.0430145263671875 -----------------------\n",
            "Straw Man 2 [Team 2] was denied cluster access by Straw Man 3 [Team 2] on turn 13.\n",
            "Straw Man 3 [Team 2] was brutally neglected by Straw Man 0 [Team 1] on turn 49.\n",
            "Straw Man 0 [Team 1] wished Asimov protected him against Straw Man 1 [Team 1] on turn 207.\n",
            "Game 50 ended on turn 207\n",
            "Game 50 runtime: 0.062299251556396484 -----------------------\n",
            "Straw Man 0 [Team 1] was swarmed by TensorBros sent by Straw Man 2 [Team 2] on turn 26.\n",
            "Straw Man 3 [Team 2] slipped and fell while running from Straw Man 1 [Team 1] on turn 227.\n",
            "Straw Man 1 [Team 1] was peer-reviewed by Straw Man 2 [Team 2] on turn 475.\n",
            "Game 51 ended on turn 475\n",
            "Game 51 runtime: 0.1629199981689453 -----------------------\n",
            "Straw Man 0 [Team 1] was brutally neglected by Straw Man 3 [Team 2] on turn 32.\n",
            "Straw Man 2 [Team 2] was denied cluster access by Straw Man 3 [Team 2] on turn 160.\n",
            "Straw Man 1 [Team 1] was forced to press Alt+F4 by Straw Man 3 [Team 2] on turn 415.\n",
            "Game 52 ended on turn 415\n",
            "Game 52 runtime: 0.12834763526916504 -----------------------\n",
            "Straw Man 3 [Team 2] got schwabbed by Straw Man 2 [Team 2] on turn 29.\n",
            "Straw Man 2 [Team 2] was denied cluster access by Straw Man 0 [Team 1] on turn 38.\n",
            "Straw Man 1 [Team 1] met a bitter end at the hands of Straw Man 0 [Team 1] on turn 115.\n",
            "Game 53 ended on turn 115\n",
            "Game 53 runtime: 0.03729057312011719 -----------------------\n",
            "Straw Man 3 [Team 2] wished Asimov protected him against Straw Man 1 [Team 1] on turn 5.\n",
            "Straw Man 2 [Team 2] wished Asimov protected him against Straw Man 0 [Team 1] on turn 9.\n",
            "Straw Man 0 [Team 1] was denied cluster access by Straw Man 1 [Team 1] on turn 125.\n",
            "Game 54 ended on turn 125\n",
            "Game 54 runtime: 0.03866434097290039 -----------------------\n",
            "Straw Man 1 [Team 1] was denied cluster access by Straw Man 2 [Team 2] on turn 39.\n",
            "Straw Man 3 [Team 2] met a bitter end at the hands of Straw Man 0 [Team 1] on turn 48.\n",
            "Straw Man 2 [Team 2] was forced to press Alt+F4 by Straw Man 0 [Team 1] on turn 55.\n",
            "Game 55 ended on turn 55\n",
            "Game 55 runtime: 0.03237485885620117 -----------------------\n",
            "Straw Man 1 [Team 1] met a bitter end at the hands of Straw Man 3 [Team 2] on turn 24.\n",
            "Straw Man 3 [Team 2] got schwabbed by Straw Man 2 [Team 2] on turn 86.\n",
            "Straw Man 0 [Team 1] was brutally neglected by Straw Man 2 [Team 2] on turn 277.\n",
            "Game 56 ended on turn 277\n",
            "Game 56 runtime: 0.08573770523071289 -----------------------\n",
            "Straw Man 3 [Team 2] had his heart stolen by Straw Man 0 [Team 1] on turn 13.\n",
            "Straw Man 2 [Team 2] was peer-reviewed by Straw Man 0 [Team 1] on turn 18.\n",
            "Straw Man 1 [Team 1] slipped and fell while running from Straw Man 0 [Team 1] on turn 255.\n",
            "Game 57 ended on turn 255\n",
            "Game 57 runtime: 0.07416057586669922 -----------------------\n",
            "Straw Man 2 [Team 2] was wasted by Straw Man 3 [Team 2] on turn 24.\n",
            "Straw Man 1 [Team 1] wished Asimov protected him against Straw Man 3 [Team 2] on turn 103.\n",
            "Straw Man 3 [Team 2] was wasted by Straw Man 0 [Team 1] on turn 184.\n",
            "Game 58 ended on turn 184\n",
            "Game 58 runtime: 0.06523418426513672 -----------------------\n",
            "Straw Man 0 [Team 1] was brutally neglected by Straw Man 3 [Team 2] on turn 11.\n",
            "Straw Man 3 [Team 2] met a bitter end at the hands of Straw Man 1 [Team 1] on turn 73.\n",
            "Straw Man 1 [Team 1] slipped and fell while running from Straw Man 2 [Team 2] on turn 195.\n",
            "Game 59 ended on turn 195\n",
            "Game 59 runtime: 0.06331419944763184 -----------------------\n",
            "Straw Man 1 [Team 1] was wasted by Straw Man 2 [Team 2] on turn 79.\n",
            "Straw Man 3 [Team 2] was swarmed by TensorBros sent by Straw Man 0 [Team 1] on turn 86.\n",
            "Straw Man 0 [Team 1] slipped and fell while running from Straw Man 2 [Team 2] on turn 335.\n",
            "Game 60 ended on turn 335\n",
            "Game 60 runtime: 0.0966336727142334 -----------------------\n",
            "Straw Man 3 [Team 2] was peer-reviewed by Straw Man 1 [Team 1] on turn 6.\n",
            "Straw Man 1 [Team 1] was wasted by Straw Man 0 [Team 1] on turn 44.\n",
            "Straw Man 2 [Team 2] was wasted by Straw Man 0 [Team 1] on turn 62.\n",
            "Game 61 ended on turn 62\n",
            "Game 61 runtime: 0.025147676467895508 -----------------------\n",
            "Straw Man 2 [Team 2] was swarmed by TensorBros sent by Straw Man 0 [Team 1] on turn 31.\n",
            "Straw Man 3 [Team 2] was denied cluster access by Straw Man 0 [Team 1] on turn 62.\n",
            "Straw Man 0 [Team 1] got schwabbed by Straw Man 1 [Team 1] on turn 283.\n",
            "Game 62 ended on turn 283\n",
            "Game 62 runtime: 0.08996772766113281 -----------------------\n",
            "Straw Man 2 [Team 2] had his heart stolen by Straw Man 3 [Team 2] on turn 8.\n",
            "Straw Man 1 [Team 1] was denied cluster access by Straw Man 0 [Team 1] on turn 27.\n",
            "Straw Man 3 [Team 2] was peer-reviewed by Straw Man 0 [Team 1] on turn 133.\n",
            "Game 63 ended on turn 133\n",
            "Game 63 runtime: 0.04053068161010742 -----------------------\n",
            "Straw Man 1 [Team 1] met a bitter end at the hands of Straw Man 2 [Team 2] on turn 33.\n",
            "Straw Man 2 [Team 2] became petrified by the sight of Straw Man 0 [Team 1] on turn 97.\n",
            "Straw Man 3 [Team 2] became petrified by the sight of Straw Man 0 [Team 1] on turn 216.\n",
            "Game 64 ended on turn 216\n",
            "Game 64 runtime: 0.0720980167388916 -----------------------\n",
            "Straw Man 3 [Team 2] was denied cluster access by Straw Man 2 [Team 2] on turn 62.\n",
            "Straw Man 2 [Team 2] was forced to press Alt+F4 by Straw Man 1 [Team 1] on turn 192.\n",
            "Straw Man 1 [Team 1] became petrified by the sight of Straw Man 0 [Team 1] on turn 221.\n",
            "Game 65 ended on turn 221\n",
            "Game 65 runtime: 0.07540059089660645 -----------------------\n",
            "Straw Man 0 [Team 1] was forced to press Alt+F4 by Straw Man 2 [Team 2] on turn 4.\n",
            "Straw Man 2 [Team 2] slipped and fell while running from Straw Man 1 [Team 1] on turn 5.\n",
            "Straw Man 1 [Team 1] was wasted by Straw Man 3 [Team 2] on turn 48.\n",
            "Game 66 ended on turn 48\n",
            "Game 66 runtime: 0.020192384719848633 -----------------------\n",
            "Straw Man 3 [Team 2] was brutally neglected by Straw Man 0 [Team 1] on turn 42.\n",
            "Straw Man 1 [Team 1] was peer-reviewed by Straw Man 2 [Team 2] on turn 75.\n",
            "Straw Man 0 [Team 1] was brutally neglected by Straw Man 2 [Team 2] on turn 140.\n",
            "Game 67 ended on turn 140\n",
            "Game 67 runtime: 0.05424308776855469 -----------------------\n",
            "Straw Man 2 [Team 2] wished Asimov protected him against Straw Man 3 [Team 2] on turn 41.\n",
            "Straw Man 3 [Team 2] was wasted by Straw Man 1 [Team 1] on turn 57.\n",
            "Straw Man 0 [Team 1] became petrified by the sight of Straw Man 1 [Team 1] on turn 67.\n",
            "Game 68 ended on turn 67\n",
            "Game 68 runtime: 0.029339075088500977 -----------------------\n",
            "Straw Man 0 [Team 1] was swarmed by TensorBros sent by Straw Man 2 [Team 2] on turn 30.\n",
            "Straw Man 2 [Team 2] was denied cluster access by Straw Man 1 [Team 1] on turn 149.\n",
            "Straw Man 1 [Team 1] slipped and fell while running from Straw Man 3 [Team 2] on turn 157.\n",
            "Game 69 ended on turn 157\n",
            "Game 69 runtime: 0.05416154861450195 -----------------------\n",
            "Straw Man 0 [Team 1] was wasted by Straw Man 3 [Team 2] on turn 23.\n",
            "Straw Man 3 [Team 2] was denied cluster access by Straw Man 1 [Team 1] on turn 47.\n",
            "Straw Man 1 [Team 1] slipped and fell while running from Straw Man 2 [Team 2] on turn 190.\n",
            "Game 70 ended on turn 190\n",
            "Game 70 runtime: 0.05693554878234863 -----------------------\n",
            "Straw Man 2 [Team 2] was swarmed by TensorBros sent by Straw Man 0 [Team 1] on turn 57.\n",
            "Straw Man 0 [Team 1] was denied cluster access by Straw Man 3 [Team 2] on turn 64.\n",
            "Straw Man 1 [Team 1] met a bitter end at the hands of Straw Man 3 [Team 2] on turn 785.\n",
            "Game 71 ended on turn 785\n",
            "Game 71 runtime: 0.21759629249572754 -----------------------\n",
            "Straw Man 0 [Team 1] got schwabbed by Straw Man 3 [Team 2] on turn 6.\n",
            "Straw Man 3 [Team 2] was peer-reviewed by Straw Man 2 [Team 2] on turn 56.\n",
            "Straw Man 2 [Team 2] wished Asimov protected him against Straw Man 1 [Team 1] on turn 71.\n",
            "Game 72 ended on turn 71\n",
            "Game 72 runtime: 0.03244805335998535 -----------------------\n",
            "Straw Man 1 [Team 1] became petrified by the sight of Straw Man 0 [Team 1] on turn 32.\n",
            "Straw Man 2 [Team 2] was forced to press Alt+F4 by Straw Man 3 [Team 2] on turn 143.\n",
            "Straw Man 3 [Team 2] wished Asimov protected him against Straw Man 0 [Team 1] on turn 151.\n",
            "Game 73 ended on turn 151\n",
            "Game 73 runtime: 0.06685996055603027 -----------------------\n",
            "Straw Man 2 [Team 2] was brutally neglected by Straw Man 0 [Team 1] on turn 11.\n",
            "Straw Man 3 [Team 2] met a bitter end at the hands of Straw Man 1 [Team 1] on turn 47.\n",
            "Straw Man 1 [Team 1] was swarmed by TensorBros sent by Straw Man 0 [Team 1] on turn 50.\n",
            "Game 74 ended on turn 50\n",
            "Game 74 runtime: 0.024324655532836914 -----------------------\n",
            "Straw Man 0 [Team 1] was wasted by Straw Man 1 [Team 1] on turn 22.\n",
            "Straw Man 2 [Team 2] slipped and fell while running from Straw Man 1 [Team 1] on turn 24.\n",
            "Straw Man 3 [Team 2] was forced to press Alt+F4 by Straw Man 1 [Team 1] on turn 184.\n",
            "Game 75 ended on turn 184\n",
            "Game 75 runtime: 0.05110526084899902 -----------------------\n",
            "Straw Man 0 [Team 1] was swarmed by TensorBros sent by Straw Man 3 [Team 2] on turn 47.\n",
            "Straw Man 3 [Team 2] slipped and fell while running from Straw Man 2 [Team 2] on turn 61.\n",
            "Straw Man 2 [Team 2] was forced to press Alt+F4 by Straw Man 1 [Team 1] on turn 98.\n",
            "Game 76 ended on turn 98\n",
            "Game 76 runtime: 0.03569364547729492 -----------------------\n",
            "Straw Man 1 [Team 1] was brutally neglected by Straw Man 3 [Team 2] on turn 8.\n",
            "Straw Man 0 [Team 1] was peer-reviewed by Straw Man 2 [Team 2] on turn 51.\n",
            "Straw Man 2 [Team 2] wished Asimov protected him against Straw Man 3 [Team 2] on turn 176.\n",
            "Game 77 ended on turn 176\n",
            "Game 77 runtime: 0.06408190727233887 -----------------------\n",
            "Straw Man 2 [Team 2] became petrified by the sight of Straw Man 0 [Team 1] on turn 50.\n",
            "Straw Man 0 [Team 1] was peer-reviewed by Straw Man 1 [Team 1] on turn 63.\n",
            "Straw Man 1 [Team 1] became petrified by the sight of Straw Man 3 [Team 2] on turn 182.\n",
            "Game 78 ended on turn 182\n",
            "Game 78 runtime: 0.057736873626708984 -----------------------\n",
            "Straw Man 3 [Team 2] was brutally neglected by Straw Man 0 [Team 1] on turn 24.\n",
            "Straw Man 2 [Team 2] became petrified by the sight of Straw Man 0 [Team 1] on turn 38.\n",
            "Straw Man 1 [Team 1] met a bitter end at the hands of Straw Man 0 [Team 1] on turn 203.\n",
            "Game 79 ended on turn 203\n",
            "Game 79 runtime: 0.05893397331237793 -----------------------\n",
            "Straw Man 0 [Team 1] became petrified by the sight of Straw Man 3 [Team 2] on turn 80.\n",
            "Straw Man 3 [Team 2] was swarmed by TensorBros sent by Straw Man 1 [Team 1] on turn 207.\n",
            "Straw Man 1 [Team 1] got schwabbed by Straw Man 2 [Team 2] on turn 223.\n",
            "Game 80 ended on turn 223\n",
            "Game 80 runtime: 0.07771587371826172 -----------------------\n",
            "Straw Man 0 [Team 1] was brutally neglected by Straw Man 3 [Team 2] on turn 27.\n",
            "Straw Man 3 [Team 2] was forced to press Alt+F4 by Straw Man 1 [Team 1] on turn 77.\n",
            "Straw Man 1 [Team 1] met a bitter end at the hands of Straw Man 2 [Team 2] on turn 236.\n",
            "Game 81 ended on turn 236\n",
            "Game 81 runtime: 0.07843494415283203 -----------------------\n",
            "Straw Man 0 [Team 1] got schwabbed by Straw Man 2 [Team 2] on turn 14.\n",
            "Straw Man 2 [Team 2] wished Asimov protected him against Straw Man 3 [Team 2] on turn 32.\n",
            "Straw Man 3 [Team 2] was swarmed by TensorBros sent by Straw Man 1 [Team 1] on turn 180.\n",
            "Game 82 ended on turn 180\n",
            "Game 82 runtime: 0.05559587478637695 -----------------------\n",
            "Straw Man 1 [Team 1] met a bitter end at the hands of Straw Man 2 [Team 2] on turn 77.\n",
            "Straw Man 0 [Team 1] had his heart stolen by Straw Man 3 [Team 2] on turn 78.\n",
            "Straw Man 3 [Team 2] slipped and fell while running from Straw Man 2 [Team 2] on turn 308.\n",
            "Game 83 ended on turn 308\n",
            "Game 83 runtime: 0.0926504135131836 -----------------------\n",
            "Straw Man 2 [Team 2] wished Asimov protected him against Straw Man 3 [Team 2] on turn 3.\n",
            "Straw Man 3 [Team 2] was wasted by Straw Man 0 [Team 1] on turn 12.\n",
            "Straw Man 0 [Team 1] had his heart stolen by Straw Man 1 [Team 1] on turn 143.\n",
            "Game 84 ended on turn 143\n",
            "Game 84 runtime: 0.05304217338562012 -----------------------\n",
            "Straw Man 1 [Team 1] was peer-reviewed by Straw Man 0 [Team 1] on turn 32.\n",
            "Straw Man 0 [Team 1] was swarmed by TensorBros sent by Straw Man 3 [Team 2] on turn 47.\n",
            "Straw Man 3 [Team 2] was denied cluster access by Straw Man 2 [Team 2] on turn 154.\n",
            "Game 85 ended on turn 154\n",
            "Game 85 runtime: 0.04674887657165527 -----------------------\n",
            "Straw Man 2 [Team 2] was forced to press Alt+F4 by Straw Man 0 [Team 1] on turn 30.\n",
            "Straw Man 1 [Team 1] had his heart stolen by Straw Man 3 [Team 2] on turn 71.\n",
            "Straw Man 3 [Team 2] had his heart stolen by Straw Man 0 [Team 1] on turn 78.\n",
            "Game 86 ended on turn 78\n",
            "Game 86 runtime: 0.03835797309875488 -----------------------\n",
            "Straw Man 3 [Team 2] got schwabbed by Straw Man 2 [Team 2] on turn 29.\n",
            "Straw Man 0 [Team 1] became petrified by the sight of Straw Man 1 [Team 1] on turn 42.\n",
            "Straw Man 1 [Team 1] was wasted by Straw Man 2 [Team 2] on turn 173.\n",
            "Game 87 ended on turn 173\n",
            "Game 87 runtime: 0.052764892578125 -----------------------\n",
            "Straw Man 2 [Team 2] was swarmed by TensorBros sent by Straw Man 3 [Team 2] on turn 13.\n",
            "Straw Man 3 [Team 2] was forced to press Alt+F4 by Straw Man 0 [Team 1] on turn 72.\n",
            "Straw Man 0 [Team 1] got schwabbed by Straw Man 1 [Team 1] on turn 350.\n",
            "Game 88 ended on turn 350\n",
            "Game 88 runtime: 0.10744714736938477 -----------------------\n",
            "Straw Man 0 [Team 1] got schwabbed by Straw Man 1 [Team 1] on turn 42.\n",
            "Straw Man 1 [Team 1] got schwabbed by Straw Man 3 [Team 2] on turn 261.\n",
            "Straw Man 2 [Team 2] was forced to press Alt+F4 by Straw Man 3 [Team 2] on turn 274.\n",
            "Game 89 ended on turn 274\n",
            "Game 89 runtime: 0.08941268920898438 -----------------------\n",
            "Straw Man 2 [Team 2] slipped and fell while running from Straw Man 3 [Team 2] on turn 68.\n",
            "Straw Man 0 [Team 1] met a bitter end at the hands of Straw Man 3 [Team 2] on turn 75.\n",
            "Straw Man 3 [Team 2] met a bitter end at the hands of Straw Man 1 [Team 1] on turn 129.\n",
            "Game 90 ended on turn 129\n",
            "Game 90 runtime: 0.050023555755615234 -----------------------\n",
            "Straw Man 2 [Team 2] slipped and fell while running from Straw Man 3 [Team 2] on turn 17.\n",
            "Straw Man 0 [Team 1] was brutally neglected by Straw Man 1 [Team 1] on turn 73.\n",
            "Straw Man 1 [Team 1] got schwabbed by Straw Man 3 [Team 2] on turn 329.\n",
            "Game 91 ended on turn 329\n",
            "Game 91 runtime: 0.1085507869720459 -----------------------\n",
            "Straw Man 2 [Team 2] became petrified by the sight of Straw Man 3 [Team 2] on turn 11.\n",
            "Straw Man 0 [Team 1] met a bitter end at the hands of Straw Man 1 [Team 1] on turn 21.\n",
            "Straw Man 1 [Team 1] was wasted by Straw Man 3 [Team 2] on turn 329.\n",
            "Game 92 ended on turn 329\n",
            "Game 92 runtime: 0.08832740783691406 -----------------------\n",
            "Straw Man 0 [Team 1] was brutally neglected by Straw Man 1 [Team 1] on turn 9.\n",
            "Straw Man 1 [Team 1] became petrified by the sight of Straw Man 3 [Team 2] on turn 55.\n",
            "Straw Man 3 [Team 2] was peer-reviewed by Straw Man 2 [Team 2] on turn 196.\n",
            "Game 93 ended on turn 196\n",
            "Game 93 runtime: 0.05678534507751465 -----------------------\n",
            "Straw Man 0 [Team 1] was wasted by Straw Man 2 [Team 2] on turn 37.\n",
            "Straw Man 2 [Team 2] was forced to press Alt+F4 by Straw Man 3 [Team 2] on turn 86.\n",
            "Straw Man 3 [Team 2] was swarmed by TensorBros sent by Straw Man 1 [Team 1] on turn 163.\n",
            "Game 94 ended on turn 163\n",
            "Game 94 runtime: 0.05376458168029785 -----------------------\n",
            "Straw Man 2 [Team 2] had his heart stolen by Straw Man 0 [Team 1] on turn 16.\n",
            "Straw Man 3 [Team 2] met a bitter end at the hands of Straw Man 0 [Team 1] on turn 48.\n",
            "Straw Man 0 [Team 1] got schwabbed by Straw Man 1 [Team 1] on turn 51.\n",
            "Game 95 ended on turn 51\n",
            "Game 95 runtime: 0.02474045753479004 -----------------------\n",
            "Straw Man 0 [Team 1] was brutally neglected by Straw Man 2 [Team 2] on turn 8.\n",
            "Straw Man 3 [Team 2] met a bitter end at the hands of Straw Man 2 [Team 2] on turn 126.\n",
            "Straw Man 2 [Team 2] was brutally neglected by Straw Man 1 [Team 1] on turn 187.\n",
            "Game 96 ended on turn 187\n",
            "Game 96 runtime: 0.05975222587585449 -----------------------\n",
            "Straw Man 3 [Team 2] wished Asimov protected him against Straw Man 2 [Team 2] on turn 6.\n",
            "Straw Man 1 [Team 1] was swarmed by TensorBros sent by Straw Man 2 [Team 2] on turn 124.\n",
            "Straw Man 0 [Team 1] was denied cluster access by Straw Man 2 [Team 2] on turn 142.\n",
            "Game 97 ended on turn 142\n",
            "Game 97 runtime: 0.04582357406616211 -----------------------\n",
            "Straw Man 2 [Team 2] wished Asimov protected him against Straw Man 0 [Team 1] on turn 11.\n",
            "Straw Man 1 [Team 1] wished Asimov protected him against Straw Man 3 [Team 2] on turn 23.\n",
            "Straw Man 0 [Team 1] slipped and fell while running from Straw Man 3 [Team 2] on turn 46.\n",
            "Game 98 ended on turn 46\n",
            "Game 98 runtime: 0.0191497802734375 -----------------------\n",
            "Straw Man 1 [Team 1] met a bitter end at the hands of Straw Man 0 [Team 1] on turn 15.\n",
            "Straw Man 3 [Team 2] was denied cluster access by Straw Man 2 [Team 2] on turn 336.\n",
            "Straw Man 0 [Team 1] was swarmed by TensorBros sent by Straw Man 2 [Team 2] on turn 439.\n",
            "Game 99 ended on turn 439\n",
            "Game 99 runtime: 0.15605950355529785 -----------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OsaoGa0TBR6u",
        "colab_type": "code",
        "outputId": "8352429b-19cc-4de0-c5c2-4f6751610e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "new = tf.trainable_variables()\n",
        "print(len(new))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hADyhiJuBXGI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "for ind, agnt in agent_dict.items():\n",
        "    # Get all the variables in the Q primary network.\n",
        "    q_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Q_primary\"+str(ind))\n",
        "    # Get all the variables in the Q target network.\n",
        "    q_target_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Q_target\"+str(ind))\n",
        "    assert len(q_vars) == len(q_target_vars)\n",
        "\n",
        "    # Hard update\n",
        "    sess.run([v_t.assign(v) for v_t, v in zip(q_target_vars, q_vars)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HGTduAMxF1hG",
        "colab_type": "code",
        "outputId": "7f57816a-8e45-47b2-aac3-9a354c0fedda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3321
        }
      },
      "cell_type": "code",
      "source": [
        "# np.random.seed(1)\n",
        "\n",
        "# Training\n",
        "sess = tf.InteractiveSession() # Initialize session\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for i_ep in range(n_ep): # Loop through games\n",
        "  \n",
        "  # Reset map and team scores\n",
        "  map, team_scores = reset(agent_dict)\n",
        "  \n",
        "  print('Game', i_ep, 'started.')\n",
        "  # For keeping track of time\n",
        "  t_start = time.time()\n",
        "  t1 = time.time()\n",
        "  for turn in range(max_turn):\n",
        "    turn_queue = action_queue(agent_dict) # Order of taking actions this turn\n",
        "\n",
        "    # End of game (last man standing)\n",
        "    if len(turn_queue) == 1:\n",
        "      print(\"David\", turn_queue[0], \"is the lone survivor.\")\n",
        "      print(\"Team Scores:\", team_scores)\n",
        "      print(\"Game\", i_ep, \"ended on turn\", turn-1)#, \"-----------------------\")\n",
        "      break\n",
        "    \n",
        "    # Target q network update\n",
        "    if turn % target_copy_freq:\n",
        "      for ind, agnt in agent_dict.items():\n",
        "        # Get all the variables in the Q primary network.\n",
        "        q_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Q_primary\"+str(ind))\n",
        "        # Get all the variables in the Q target network.\n",
        "        q_target_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"Q_target\"+str(ind))\n",
        "        assert len(q_vars) == len(q_target_vars)\n",
        "\n",
        "        # Hard update\n",
        "        sess.run([v_t.assign(v) for v_t, v in zip(q_target_vars, q_vars)])\n",
        "\n",
        "\n",
        "    # Loop through alive agents\n",
        "    for j in turn_queue:\n",
        "      # Agent object\n",
        "      current_agent = agent_dict[j]\n",
        "      \n",
        "      # If dead in queue, pass turn.\n",
        "      if not current_agent.alive():\n",
        "        continue;\n",
        "\n",
        "      state = current_agent.observe(map)    # State formation\n",
        "      \n",
        "      # Action selection\n",
        "      q_values = sess.run(current_agent.DQN.outputs,\n",
        "                          feed_dict = {current_agent.DQN.inputs:np.reshape(state,[1, state.size])}) # Get q-values from the network\n",
        "      p_values = tf.exp(beta * q_values)/tf.reduce_sum(tf.exp(beta * q_values)) # Boltzmann probabilities\n",
        "      action = np.random.choice(np.arange(4),p=np.squeeze(p_values.eval()))\n",
        "\n",
        "      dir = get_dir(action) # Convert to direction\n",
        "\n",
        "      # Take the action\n",
        "      personal_point,team_point = current_agent.act(dir, turn, quiet=False)\n",
        "      \n",
        "      # New state\n",
        "      state_new = current_agent.observe(map)\n",
        "\n",
        "      # Immediate reward\n",
        "      reward = sigma * personal_point + (1-sigma) * team_point\n",
        "      \n",
        "      # Add experience to memory: [s,a,r,s']\n",
        "      current_agent.memory.add([state, action, reward, state_new])\n",
        "\n",
        "      # Update Q-function\n",
        "      if not current_agent.tenure:\n",
        "        batch = current_agent.memory.sample(batch_size)\n",
        "        actual_batch_size = len(batch)\n",
        "\n",
        "        # Separate out s,a,r,s' from the batch which is a list of lists\n",
        "        states = np.array([item[0] for item in batch])\n",
        "        actions = np.array([item[1] for item in batch])\n",
        "        rewards = np.array([item[2] for item in batch])\n",
        "        next_states = np.array([item[3] for item in batch])\n",
        "\n",
        "        # Reshape (-1 automatically determines the appropriate length)\n",
        "        states = np.reshape(states,[actual_batch_size, -1])\n",
        "        actions = np.reshape(actions,[actual_batch_size, -1])\n",
        "        rewards = np.reshape(rewards,[actual_batch_size, -1])\n",
        "        next_states = np.reshape(next_states,[actual_batch_size, -1])\n",
        "        \n",
        "        # Q-value of the next state\n",
        "        Q_next = sess.run(current_agent.DQN_target.outputs,\n",
        "                          feed_dict = {current_agent.DQN_target.inputs:next_states})\n",
        "        Q_next_max = np.max(Q_next,axis=1,keepdims=True) # We want to apply np.max() to Q_next\n",
        "        \n",
        "        target_Qs = rewards + gamma * Q_next_max\n",
        "\n",
        "        loss, _ = sess.run([current_agent.DQN.loss, current_agent.DQN.optimizer],\n",
        "                            feed_dict={current_agent.DQN.inputs: states,\n",
        "                                       current_agent.DQN.target_Q: target_Qs,\n",
        "                                       current_agent.DQN.actions: actions})\n",
        "        \n",
        "        '''\n",
        "        # Write TF Summaries\n",
        "        summary = sess.run(write_op, feed_dict={current_agent.DQN.inputs: states,\n",
        "                                           current_agent.DQN.target_Q: target_Qs\n",
        "                                           current_agent.DQN.actions: actions})\n",
        "        writer.add_summary(summary, i_ep)\n",
        "        writer.flush()\n",
        "        '''\n",
        "    \n",
        "    # Graphics -----------------------------------------------------------------\n",
        "    t2 = time.time()\n",
        "    print(\"Runtime:\", t2-t1)\n",
        "    t1 = t2\n",
        "    \n",
        "    print(\"Game \" + str(i_ep) + \", turn \" + str(turn))\n",
        "    plot_map(map)\n",
        "    time.sleep(0.5)\n",
        "\n",
        "    \n",
        "    \n",
        "    # Game ended without conclusion\n",
        "    if turn == max_turn-1:\n",
        "      print(\"Game did not finish.\")\n",
        "\n",
        "  t_finish = time.time()\n",
        "  print(\"Game\", i_ep, \"runtime:\", t_finish-t_start, \"-----------------------\")\n",
        "  #print(\"Game\", str(i_ep), \"ended on turn\", turn, \"-----------------------\")\n",
        "    \n",
        "sess.close() # Close session"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Runtime: 2.25824236869812\n",
            "Game 0, turn 228\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-88-c28c5b791f9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Game \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_ep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", turn \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mturn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mplot_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-d1fd97fe1556>\u001b[0m in \u001b[0;36mplot_map\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;31m# Display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(*objs, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mpublish_display_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mformat_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mformat_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;31m# nothing to display (e.g. _ipython_display_ took over)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self, obj, include, exclude)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0;31m# FIXME: log the exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-9>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36mcatch_format_error\u001b[0;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"show traceback on failed format call\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;31m# don't warn on NotImplementedErrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, **kwargs)\u001b[0m\n\u001b[1;32m   2214\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m                     \u001b[0mdryrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2216\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2217\u001b[0m                 \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cachedRenderer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m                 \u001b[0mbbox_inches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tightbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0moriginal_dpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;31m# if toolbar:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;31m#     toolbar.set_cursor(cursors.WAIT)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# if toolbar:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1299\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2435\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2437\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2439\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m             im, l, b, trans = self.make_image(\n\u001b[0;32m--> 566\u001b[0;31m                 renderer, renderer.get_image_magnification())\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mmake_image\u001b[0;34m(self, renderer, magnification, unsampled)\u001b[0m\n\u001b[1;32m    791\u001b[0m         return self._make_image(\n\u001b[1;32m    792\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_bbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagnification\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m             unsampled=unsampled)\n\u001b[0m\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_unsampled_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_make_image\u001b[0;34m(self, A, in_bbox, out_bbox, clip_bbox, magnification, unsampled, round_to_pixel_border)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;31m# (of int or float)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;31m# or an RGBA array of re-sampled input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m             \u001b[0;31m# output is now a correctly sized RGBA array of uint8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cm.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(self, x, alpha, bytes, norm)\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrgba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, alpha, bytes)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0mlut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'clip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrgba\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'scalar'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgba\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAA3FJREFUeJzt3bFNXEEYRtFZyxJFIPfhDtzJdmEh\nt7F9OHAH7sOiCKLniMQaMZHfXJZzQgi+B9LVPkTwX47jGEDPp90PAMyJE6LECVHihChxQtTnt755\nvLwcl4eHs54FPqrL9IuLf6Ucz49f/s/j/OPx+c+43W6nbI0xxvV6PW3Plq3F3jROr7UQJU6IEidE\niROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFC\nlDghSpwQ9eY5hjHG+Pn0/YznGNdTVuD9WJ5jOOtB4AObnmNYfnLe8X2Ku/zZbL2vrde9GX9zQpQ4\nIUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJ\nE6LECVHihChxQpRzDLCfcwy79mzZWu3NeK2FKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFK\nnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKOcYYD/nGHbt2bK12pvxWgtR\n4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQ\nJU6IEidEiROixAlRzjHAfs4x7NqzZWu1N+O1FqLECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKE\nKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6KcY4D9nGPYtWfL1mpvxmst\nRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChx\nQpQ4IUqcECVOiBInRDnHAPs5x7Brz5at1d6M11qIEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJ\nE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEuZUC+7mVsmvvnre+\nPv06Zev3j293+Tt83ZvxWgtR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBIn\nRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlRq3MMwCY+OSFKnBAlTogSJ0SJE6LECVF/AYxk\n7dDhjP+MAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fb0cb065ac8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "LuWc2j_rKPBR",
        "colab_type": "code",
        "outputId": "98cfebe3-2bb9-493a-ff96-164fcd6f0f55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "current_agent = agent_dict[3]\n",
        "len(current_agent.memory.buffer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    }
  ]
}